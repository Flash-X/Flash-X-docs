\chapter{Particles Unit}
\label{Chp:Particles}

The support for particles in \flashx comes in two
flavors, \emph{active} and \emph{passive}. Active particles are
further classified into two categories; \emph{massive} and
\emph{charged}. The active particles contribute to the dynamics of the
simulation, while passive particles follow the motion of Lagrangian
tracers and make no contribution to the dynamics. Particles are
dimensionless objects characterized by positions ${\bf x}_i$,
velocities ${\bf v}_i$, and sometimes other quantities  such as mass
$m_i$ or charge $q_i$~. Their characteristic quantities are considered
to be defined at their positions and may be set by interpolation from
the mesh or may be used to define mesh quantities by
extrapolation. They move relative to the mesh and can travel from
block to block, requiring  communication patterns different from those
used to transfer boundary information between processors for mesh-based
data. 

Passive particles acquire their kinematic information (velocities)
directly from the mesh. They are meant to be used as passive flow tracers and
do not make sense outside of a hydrodynamical context. The governing
equation for the $i$th passive particle is particularly simple and
requires only the time integration of interpolated mesh
velocities. \begin{equation}\label{Eqn:particle_motion_velocity}
{d{\bf x}_i\over dt}    =  {\bf v}_i 
\end{equation}
Active particles experience forces and may themselves contribute to
the problem dynamics (\eg, through long-range forces or through
collisions). They may additionally have their own motion independent
of the grid, so an additional motion equation of 
    \begin{equation}
    \label{Eq:particle_motion_acceleration}
      {\bf v}_i^{n+1} = {\bf v}_i^n + {\bf a}_i^n\Delta t^n ~.
    \end{equation}
may come into play. Here ${\bf a}_i$ is the particle
acceleration. Solving for the motion of active particles is also
referred to as solving the $N$-body problem. The equations of motion
for the $i$th active particle include the equation
\eqref{Eqn:particle_motion_velocity} and another describing the
effects of forces. 

\begin{equation} 
\label{Eqn:particle_motion_forces}
m_i{d{\bf v}_i\over dt} =  {\bf F}_{{\rm lr,}i} + {\bf F}_{{\rm sr,}i}\ ,
\end{equation}
Here, ${\bf F}_{{\rm lr,}i}$ represents the sum of all long-range forces
(coupling all particles, except possibly those handled by the short-range term)
acting on the $i$th particle and ${\bf F}_{{\rm sr,}i}$ represents the
sum of all short-range forces (coupling only neighboring particles)
acting on the particle.

For both types of particles, the primary challenge is to integrate
\eqref{Eqn:particle_motion_velocity} forward through time. Many
alternative integration methods are described in
Section~\secref{Sec:Particles Integration} below.  Additional
information about the mesh to particle mapping is described in
\secref{Sec:Particles Mapping}.  An introduction to the particle
techniques used in Flash-X is given by R.~W.~Hockney and J.~W.~Eastwood
in {\it Computer Simulation using Particles} (Taylor and Francis,
1988).

\begin{flashtip}
Please note that the particles routines have not been thoroughly tested
with non-Cartesian coordinates; use them at your own risk! 
\end{flashtip}

\begin{flashtip}[New since \flashx]
Since release 3.1 of Flash-X, a single simulation can have both active
and passive particles defined.  \flashx and \flashx allowed
only active {\em or} passive particles in a simulation.  Because
of the added complexity, new \code{Config} syntax and new \setup
script syntax is necessary for Particles.  See
 \secref{Sec:ListSetupArgs} for command line options,
\secref{Sec:FlashHparticles} for \code{Config} sytax,
and \secref{Sec:ParticlesUsing} below for more details.
\end{flashtip}

\flashx includes support for \emterm{sink particles}. These are a
special kind of (massive) active particles, with special rules for
creation, mass accretion, and interaction with fluid variables and
other particles. See \secref{Sec:Particles Sink} below for information
specific to sink particles.










\section{Time Integration}
\label{Sec:Particles Integration}

The active and passive particles have many different time
integration schemes available. 
The subroutine
\api{Particles/Particles_advance} handles the 
movement of particles through space and time. 
%% Because \flashx has support for including both active
%% and passive particles in a single simulation, \code{Particles\_advance}
%% calls helper routines \code{pt_advanceActive} and \code{pt_advancePassive}.
%% Only one type of passive and one type of active time integration scheme can be
%% selected for any simulation, no matter how many types of active particles exist.
Because \flashx has support for including different types of both active
and passive particles in a single simulation, the implementation of
\api{Particles/Particles_advance} may call several
helper routines of the form \code{pt\_advance\metavar{METHOD}} (\eg,
\code{pt\_advanceLeapfrog}, \code{pt\_advanceEuler\_active}, \code{pt\_advanceCustom}),
each acting on an appropriate subset of existing particles.
The \metavar{METHOD} here is determined by the \code{ADVMETHOD} part of the
\code{PARTICLETYPE} \code{Config} statement (or the
\code{ADV} par of a \code{-particlemethods} \setup option) for the type of particle. 
See the \code{Particles\_advance} source code for the mapping from 
\code{ADVMETHOD} keyword to \code{pt\_advance\metavar{METHOD}} subroutine call.


\subsection{Active Particles (Massive)}
\label{Sec:Active Partices Integration}

The \code{active} particles implementation includes different
time integration schemes, long-range force laws (coupling all
particles), and short-range force laws (coupling nearby particles).
 The attributes listed in \tblref{Tab:active particle attributes} are
provided by this subunit.
A specific implementation of the active portion of
\api{Particles/Particles_advance} 
 is selected by a setup option such
as \code{-with-unit=Particles/\-ParticlesMain/\-active/\-massive/\-Leapfrog}, or by
specifying something like \code{REQUIRES
Particles/\-ParticlesMain/active/\-massive/\-Leapfrog} in a simulation's
\code{Config} file (or by listing the path in the \code{Units} file if
not using the \code{-auto} configuration option).  
Further details are given is \secref{Sec:ParticlesUsing} below.

Available time integration schemes for active particles include
\begin{itemize}
\item {\bf Forward Euler.} Particles are
      advanced in time from $t^n$ to $t^{n+1} = t^n + \Delta t^n$
      using the following difference equations:
      \begin{eqnarray}
      \nonumber
      {\bf x}_i^{n+1} &= {\bf x}_i^n + {\bf v}_i^n\Delta t^n\\
      {\bf v}_i^{n+1} &= {\bf v}_i^n + {\bf a}_i^n\Delta t^n ~.
      \end{eqnarray}
      Here ${\bf a}_i$ is the particle acceleration.  Within \flashx, this scheme
    is implemented in \code{Particles/\-ParticlesMain/\-active/\-massive/\-Euler}.
    This Euler scheme (as well 
    as the Euler scheme for the passive particles) is first-order accurate and is
    included for testing purposes only. It should not be used in a production run.


\item {\bf Variable-timestep leapfrog.} Particles
      are advanced using the following difference equations
      \begin{eqnarray}
      \nonumber
      {\bf x}_i^1       &= {\bf x}_i^0 + {\bf v}_i^0\Delta t^0\\
      {\bf v}_i^{1/2}   &= {\bf v}_i^0 + {1\over2}{\bf a}_i^0\Delta t^0\\
      \nonumber
      {\bf v}_i^{n+1/2} &= {\bf v}_i^{n-1/2} + C_n{\bf a}_i^n +
                            D_n{\bf a}_i^{n-1}\\
      \nonumber
      {\bf x}_i^{n+1}   &= {\bf x}_i^n + {\bf v}_i^{n+1/2}\Delta t^n~.
      \end{eqnarray}      The coefficients $C_n$ and $D_n$ are given by
      \begin{eqnarray}
      \nonumber
      C_n &= {1\over2}\Delta t^n + {1\over3}\Delta t^{n-1} +
              {1\over6}\left({{\Delta t^n}^2\over\Delta t^{n-1}}\right)\\
      D_n &= {1\over6}\left(\Delta t^{n-1} -
              {{\Delta t^n}^2\over\Delta t^{n-1}}\right)\ .
      \end{eqnarray}
      By using time-centered velocities and stored accelerations, this method
      achieves second-order time accuracy even with variable timesteps.
Within \flashx, this scheme
    is implemented in \code{Particles/\-ParticlesMain/\-active/\-massive/\-Leapfrog}

\item {\bf Cosmological variable-timestep leapfrog.}
 (\code{Particles/\-ParticlesMain/\-active/\-massive/\-LeapfrogCosmo})
      The coefficients in the leapfrog update are modified to take into
      account the effect of cosmological redshift on the particles.
      The particle positions ${\bf x}$ are interpreted as comoving
      positions, and the particle velocities ${\bf v}$ are interpreted
      as comoving peculiar velocities (${\bf v} = {\dot{\bf x}}$).
      The resulting update steps are
%\vfill
%\eject
      \begin{eqnarray}
      \nonumber
      {\bf x}_i^1       &=& {\bf x}_i^0 + {\bf v}_i^0\Delta t^0\\
      \nonumber
      {\bf v}_i^{1/2}   &=& {\bf v}_i^0 + {1\over2}{\bf a}_i^0\Delta t^0\\
      \nonumber
      {\bf v}_i^{n+1/2} &=& {\bf v}_i^{n-1/2}
                            \left[ 1 - {A^n\over 2} \Delta t^n
                                   + {1\over 3!} {\Delta t^n}^2
                                   \left( {A^n}^2 -\dot A^n \right)\right]
                            \left[ 1 - \Delta t^{n-1} {A^n\over 2}
                                   + {\Delta t^{n-1}}^2 {{A^n}^2+
                                   2\dot A^n\over 12} \right]\nonumber \\
      && + {\bf a}_i^n \left[ {\Delta t^{n-1}\over 2} +
                                  {{\Delta t^n}^2\over
                                   6\Delta t^{n-1}} + {\Delta t^{n-1} \over 3}
                                   - { A^n \Delta t^n \over 6}
                                   \left( \Delta t^n + \Delta t^{n-1} \right)
                                   \right] \nonumber \\
      \nonumber
      && + {\bf a}_i^{n-1} \left[ {{\Delta t^{n-1}}^2 - {\Delta t^n}^2
                                   \over 6\Delta t^{n-1}}
                                   - { A^n \Delta t^{n-1} \over 12}
                                   (\Delta t^n + \Delta t^{n-1}) \right] \\
      \nonumber
      {\bf x}_i^{n+1} &=& {\bf x}_i^n + {\bf v}_i^{n+1/2} \Delta t^n\ .
      \end{eqnarray}
      Here we define $A \equiv -2 \dot a/a$, where $a$ is the scale factor.
      Note that the acceleration ${\bf a}_i^{n-1}$ from the previous timestep
      must be retained in order to obtain second order accuracy.
      Using the \code{Particles/\-ParticlesMain/\-passive/\-LeapfrogCosmo} time integration
      scheme only makes sense if the \code{Cosmology} module is also
      used, since otherwise $a \equiv 1$ and ${\dot a} \equiv 0$.
      
% \item{\textbf{Estimated Midpoint (\code{Particles/\-ParticlesMain/passive/\-EstiMidpoint}).}}
% This is a corrected version of what used to be known as Predictor-Corrector in \flashx and pre-release
% distributions of the \flashx code.
% The \emterm{Estimated Midpoint} scheme is second order when time steps remain constant.  
% Particle advancement follows the equation
% \newcommand{\XP}{\textbf{xp(t+1.5*dtNew)}}
% \newcommand{\VP}{\textbf{vp(t+1.5*dtNew)}}
% \newcommand{\Deltat}{{\Delta t}}
% \catcode`\_=8
% \newcommand{\xn}{{\bf x}_i^{n}}
% \newcommand{\vn}{{\bf v}_i^{n}}
% \newcommand{\tn}{t^{n}}
% \newcommand{\tp}{t_{*}^{n+\frac12}}
% \newcommand{\tP}{t^n + \Deltatp }
% \newcommand{\thalf}{t^{n+\frac12}}
% \newcommand{\tH}{t^n + \frac12 \Delta t^{n} }
% %\newcommand{\Deltatp}{\Delta t^{*,n-\frac12} }
% \newcommand{\Deltatp}{\Delta t_{*}^{n} }
% \newcommand{\DeltatP}{\frac12 \Delta t^{n-1} }
% \newcommand{\Deltathalf}{\frac12 \Delta t^{n}}
% \newcommand{\xh}{{\bf x}_i^{{\mathrm{E}},n+\frac12}}
% \newcommand{\xp}{{\bf x}_i^{*,n+\frac12}}
% \newcommand{\vp}{{\bf v}_i^{*,n+\frac12}}
% %begin{latexonly}
% \makeUsNormal
% %end{latexonly}
%      \begin{eqnarray}
%     \label{Eqn:EstiMidpoint}
%       {\bf x}_i^{n+1} &=& {\bf x}_i^n + 
%   \frac{\Delta t^n}{2}  \left[ {\bf v}_i^{*,n+\frac{1}{2}} + {\bf v}(\xp,t^{n+1}) \right]      \;,\\
%       \hbox{where}\quad \vp &=& {\bf v}(\xp,t^{n})    \;,\nonumber\\
%       \xp &=& {\bf x}_i^n + \frac12 \Delta t^{n-1} {\bf v}_i^n  \quad.\nonumber
%       \end{eqnarray}
% Here, ${\bf v}_i^{*,n+\frac{1}{2}}$ is a predicted velocity, evaluated at $t^n$ at a point $\xp$
% estimated (at $t^n$) to be the midpoint that will be reached halfway between $t^n$ and $t^{n+1}$.
% The term ${\bf v}(\xp,t^{n+1})$ represents another evaluation at the same position but
% at time $t^{n+1}$. The velocity at the midpoint at time $t + \frac{\Delta t}{2}$, which is not
% directly available for evaluation, is thus estimated as the average of two evaluations at
% the two closest times that are available. 
% 
% Note that the timestep that is used to compute the estimated midpoint position
% $\xp$ is not $\frac12\Deltat^n$ (which is
% exactly half of the length of the interval $[t^n,t^{n+1}]$ and thus should be used),
% but it is $\frac12\Deltat^{n-1}$, half of the lenght of the previous time interval
% $[t^{n-1},t^n]$ \footnote{The provided implementations of
% \api{Driver/Driver_evolveFlash} call 
% \api{Driver/Driver_computeDt} to compute the next time step $\Deltat^{n+1}$ only after
% all physics units, including \unit{Particles}, are done with advancing the solution
% to $t^{n+1}$. A reason for this is that the new time step may depend on the updated
% solution (including, in principle, the state of particles). Thus $\Deltat^{n+1}$
% is not available in the \code{Particles_advance} call that advances particles
% from $t^n$ to $t^{n+1}$ and which has to compute ${\bf x}_i^{*,n+\frac32}$
% and ${\bf v}_i^{*,n+\frac32} = {\bf v}({\bf x}_i^{*,n+\frac32},t^{n+1})$
% for use in the next iteration.}.
% When the time step changes, the actual midpoint time will deviate from
% the one used in the estimate.
% 
% 
% The scheme is second order if $\Delta t^n = \Delta t^{n+1}$.
% When timesteps $\Delta t$ are changing,
% the accuracy of the method is less than second order.
% An Euler step, as described for \code{EulerActive} in \eqref{Eqn:particle_passive_euler}, 
%  is taken the first time when
% \code{Particles/ParticlesMain/EstiMidpoint2Passive/Particles\_advance} is called and whenever the
% time step has changed too much. The timestep is considered to have changed too much
% if the following is true:
% \begin{displaymath}
%     \left| \Delta t^{n} - \Delta t^{n-1} \right| \geq
%                                                       \code{pt\_dtChangeTolerance} \times  \Delta t^{n-1},
% \end{displaymath}
% where \rpi{Particles/pt\_dtChangeTolerance} is a runtime parameter.
% 
% 
% The \code{passive/EstiMidpoint} \childunit uses the following additional particle attributes
% for storing the values of $\xp$ and $\vp$ between the \code{Particles_advance} calls at $t^n$
% and $t^{n+1}$:
% \begin{codeseg}
% PARTICLEPROP velPredX REAL
% PARTICLEPROP velPredY REAL
% PARTICLEPROP velPredZ REAL
% PARTICLEPROP posPredX REAL
% PARTICLEPROP posPredY REAL
% PARTICLEPROP posPredZ REAL
% \end{codeseg}
% 
% in second-order time accuracy.

\item {\bf Sink particles} have their own implementation of
  several advancement methods (with time subcycling), implemented under    
  \code{Particles/\-ParticlesMain/\-active/\-Sink},
  see description below in \secref{Sec:Particles Sink}.
\end{itemize}

The leapfrog-based integrators implemented under
\code{Particles/\-ParticlesMain/\-active/\-massive}
supply the additional particle
attributes listed in \tblref{Tab:leapfrog attributes}. 

\begin{center}
\begin{longtable}{llp{2in}}
\caption{ Particle attributes  provided by active particles. } 
\label{Tab:active particle attributes} \\ 
Attribute         & Type     & Description\\
\hline
\code{MASS_PART_PROP}   & REAL     & Particle mass\\
\code{ACCX_PART_PROP} & REAL     & $x$-component of particle acceleration\\
\code{ACCY_PART_PROP} & REAL     & $y$-component of particle acceleration\\
\code{ACCZ_PART_PROP} & REAL     & $z$-component of particle acceleration\\
\hline

\end{longtable}
\end{center}

\begin{center}
\begin{longtable}{llp{2in}}
\caption{Particle attributes  provided by leapfrog time integration.} \\
\label{Tab:leapfrog attributes} 
Attribute          & Type     & Description\\
\hline
\code{OACX_PART_PROP}
                       & REAL     & $x$-component of particle acceleration
                                    at previous timestep\\
\code{OACY_PART_PROP}
                       & REAL     & $y$-component of particle acceleration
                                    at previous timestep\\
\code{OACZ_PART_PROP}
                       & REAL     & $z$-component of particle acceleration
                                    at previous timestep\\
\hline
\end{longtable}
\end{center}


\subsection{Charged Particles - Hybrid PIC}
\label {Sec: hybridPIC}

% gres should be GENERIC
% Have Fat FD as branch?  So Flash-X version = mine
% Remove grpe (can be computed)
% Clean code and put in 3.3?


Collisionless plasmas are often modeled using fluid 
magnetohydrodynamics (MHD) models.  However, the MHD fluid approximation is
questionable when the gyroradius of the ions is large compared to the 
spatial region that is studied.  On the other hand, kinetic models that 
discretize the full velocity space, or full particle in cell (PIC) models 
that treat ions and electrons as particles, are computationally very expensive. 
For problems where the time scales and spatial scales of the ions are of interest, 
hybrid models provide a compromise.  In such models, the ions are 
treated as discrete particles, while the electrons are treated as a 
(often massless) fluid.  This mean that the time scales and spatial 
scales of the electrons do not need to be resolved, and enables applications such as 
modeling of the solar wind interaction with planets. 
For a detailed discussion of different plasma models, see Ledvina et al.~(2008).

\subsubsection{The hybrid equations}
In the hybrid approximation, ions are treated as particles, 
and electrons as a massless fluid. 
In what follows we use SI units. 
We have $N_I$ ions at positions $\mathbf{r}_i(t)$~[m] with velocities 
$\mathbf{v}_i(t)$~[m/s], mass $m_i$~[kg] and charge $q_i$~[C], 
$i=1,\ldots,N_I$. 
By spatial averaging we can define the 
charge density $\rho_I(\mathbf{r},t)$~[Cm$^{-3}$] of the ions, 
their average velocity $\mathbf{u}_I(\mathbf{r},t)$~[m/s], and 
the corresponding current density 
$\mathbf{J}_I(\mathbf{r},t)=\rho_I\mathbf{u}_I$~[Cm$^{-2}$s$^{-1}$]. 
Electrons are modelled as a fluid with charge density 
$\rho_e(\mathbf{r},t)$, average velocity $\mathbf{u}_e(\mathbf{r},t)$, 
and current density $\mathbf{J}_e(\mathbf{r},t)=\rho_e\mathbf{u}_e$. 
The electron number density is $n_e=-\rho_e/e$, where $e$ is 
the elementary charge. 
If we assume that the electrons are an ideal gas, 
then $p_e=n_ekT_e$, so the pressure is directly related to temperature 
($k$ is Boltzmann's constant).

The trajectories of the ions are computed from the Lorentz force, 
\[
  \dfrac{d\mathbf{r}_i}{dt} = \mathbf{v}_i, \quad
  \dfrac{d\mathbf{v}_i}{dt} = \dfrac{q_i}{m_i} \left( 
    \mathbf{E}+\mathbf{v}_i\times\mathbf{B} \right), \quad
    i=1,\ldots,N_I
\]
where $\mathbf{E}=\mathbf{E}(\mathbf{r},t)$ is the electric field, 
and $\mathbf{B}=\mathbf{B}(\mathbf{r},t)$ 
is the magnetic field.  The electric field is given by 
\[
  \mathbf{E} = \dfrac{1}{\rho_I} \left( -\mathbf{J}_I\times\mathbf{B} 
  +\mu_0^{-1}\left(\nabla\times\mathbf{B}\right) \times \mathbf{B} 
   - \nabla p_e \right) + \eta \mathbf{J} 
   - \eta_h \nabla^2 \mathbf{J}, 
\]
where $\rho_I$ is the ion charge density, 
$\mathbf{J}_I$ is the ion current, 
$p_e$ is the electron pressure, 
$\mu_0=4\pi\cdot10^{-7}$ is the magnetic constant, 
$\mathbf{J}=\mu_0^{-1}\nabla\times\mathbf{B}$ is the current, 
and $\eta_h$ is a hyperresistivity. 
Here we assume that $p_e$ is adiabatic.  Then the relative change in 
electron pressure is related to the relative change in electron density by 
\[
  \frac{p_e}{p_{e0}} = \left( \frac{n_e}{n_{e0}} \right)^{\gamma} , 
\]
where the zero subscript denote reference values (here the 
initial values at $t=0$). 
Then Faraday's law is used to advance the magnetic field in time, 
        \[
          \pder{\mathbf{B}}{t} = -\nabla\times\mathbf{E}. 
        \]


\subsubsection{A cell-centered finite difference hybrid PIC solver}
We use a cell-centered representation of the magnetic field on a uniform 
grid.  All spatial derivatives are discretized using standard second order 
finite difference stencils.  
Time advancement is done by a predictor-corrector leapfrog method 
with subcycling of the field update, denoted cyclic leapfrog (CL) 
by Matthew (1994). An advantage of the discretization is that the 
divergence of the magnetic field is zero, down to round off errors. 
The ion macroparticles (each representing a large number of real particles) are deposited 
on the grid by a cloud-in-cell method (linear weighting), and 
interpolation of the fields to the particle positions are done by 
the corresponding linear interpolation. 
Initial particle positions are drawn from a uniform distribution, 
and initial particle velocities from a Maxwellian distribution. 
Further details of the algorithm can be found in Holmstr\"{o}m, M. (2012,2013)
and references therein, where 
an extension of the solver that include inflow and outflow 
boundary conditions was used to model the interaction between 
the solar wind and the Moon. 
%% External fields
%% (Non-uniform) resistivity
In what follows we describe the Flash-X implementation of 
the hybrid solver with periodic boundary conditions. 


\subsubsection{Hybrid solver implementation}
The two basic operations needed for a PIC code are 
provided as standard operations in Flash-X: 
\begin{itemize}
\item Deposit charges and currents onto the grid: 
      \texttt{call Grid\_mapParticlesToMesh()}
\item Interpolate fields to particle positions: 
      \texttt{call Grid\_mapMeshToParticles()}
\end{itemize}
At present the solver is restricted to a Cartesian uniform grid, since 
running an electromagnetic particle code on an adaptive grid 
is not straightforward.  Grid refinement/coarsening must be 
accompanied by particle split/join operations. 
Also, jumps in the cell size can lead to reflected electromagnetic waves. 

The equations are stated in SI units, so all parameters 
should be given in SI units, and all output will be in SI units. 
The initial configuration is a spatial uniform plasma consisting of 
two species.  The first species, named \texttt{pt\_picPname\_1} 
consists of particles of mass \texttt{pt\_picPmass\_1} and charge 
\texttt{pt\_picPcharge\_1}.  
The initial (at $t=0$) uniform number density is \texttt{pt\_picPdensity\_1}, 
and the velocity distribution is Maxwellian with a drift velocity of 
(\texttt{pt\_picPvelx\_1}, \texttt{pt\_picPvely\_1}, \texttt{pt\_picPvelz\_1}), 
and a temperature of \texttt{pt\_picPtemp\_1}. 
Each model macro-particle represents many real particles. 
The number of macro-particles per grid cell at the start of the simulation 
is set by \rpi{Particles/pt_picPpc_1}. 
So this parameter will control the total number of macro-particles 
of species 1 in the simulation. 

All the above parameters are also available for a second species, e.g., 
\texttt{pt\_picPmass\_2}, 
which is initialized in the same way as the first species. 
\begin{table}[ht]
\caption{Runtime parameters for the hybrid solver. 
         Initial values are at $t=0$.
         For each parameter for species 1, there is a corresponding 
         parameter for species 2 (named with 2 instead of 1), 
         e.g., \texttt{pt\_picPvelx\_2}.}
\begin{center}
\begin{tabular}{lllp{14em}}
Variable & Type & Default & Description \\
\hline 
\texttt{pt\_picPname\_1} & STRING & "H+" & Species 1 name \\
\texttt{pt\_picPmass\_1} & REAL & 1.0 & Species 1 mass, $m_i$ [amu] \\
\texttt{pt\_picPcharge\_1} & REAL & 1.0 & Species 1 charge, $q_i$ [e] \\
\texttt{pt\_picPdensity\_1} & REAL & 1.0 & Initial $n_I$ species 1 [m$^{-3}$] \\
\texttt{pt\_picPtemp\_1} & REAL & 1.5e5 & Initial $T_I$ species 1 [K] \\
\texttt{pt\_picPvelx\_1} & REAL & 0.0 & Initial $\mathbf{u}_I$ species 1 [m/s] \\
\texttt{pt\_picPvely\_1} & REAL & 0.0 & \\
\texttt{pt\_picPvelz\_1} & REAL & 0.0 & \\
\texttt{pt\_picPpc\_1} & REAL & 1.0 & Number of macro-particle of species 1 per cell \\
\texttt{sim\_bx} & REAL & 0.0 & Initial $\mathbf{B}$ (at $t=0$) [T] \\
\texttt{sim\_by} & REAL & 0.0 &  \\
\texttt{sim\_bz} & REAL & 0.0 &  \\
\texttt{sim\_te} & REAL & 0.0 & Initial $T_e$ [K] \\
\texttt{pt\_picResistivity} & REAL & 0.0 &  Resistivity, $\eta$ [$\Omega$ m]\\
\texttt{pt\_picResistivityHyper} & REAL & 0.0 & Hyperresistivity, $\eta_h$ \\
\texttt{sim\_gam} & REAL & -1.0 & Adiabatic exponent for electrons \\
\texttt{sim\_nsub} & INTEGER & 3 & Number of CL B-field update subcycles (must be odd) \\
\texttt{sim\_rng\_seed} & INTEGER & 0 & Seed the random number generator (if $>0$) 
\end{tabular}
\end{center}

\end{table}
The grid is initialized with the uniform magnetic field 
(\texttt{sim\_bx}, \texttt{sim\_by}, \texttt{sim\_bz}). 

%% Table with VARIABLES (see UG p.157) and PROPERTIES from Config

Now for grid quantities. 
The cell averaged mass density is stored in \texttt{pden}, and the 
charge density in \texttt{cden}. 
The magnetic field is represented by 
(\texttt{grbx}, \texttt{grby}, \texttt{grbz}). 
A background field can be set during initialization in 
(\texttt{gbx1}, \texttt{gby1}, \texttt{gbz1}). 
We then solve for the deviation from this background field. 
Care must be take so that the background field is divergence free, 
using the discrete divergence operator. 
The easiest way to ensure this is to compute the background field 
as the rotation of a potential.  
The electric field is stored in 
(\texttt{grex}, \texttt{grey}, \texttt{grez}), the current density in 
(\texttt{grjx}, \texttt{grjy}, \texttt{grjz}) , and the ion current density in 
(\texttt{gjix}, \texttt{gjiy}, \texttt{gjiz}). 
A resistivity is stored in \texttt{gres}, thus it is possible to have 
a non-uniform resistivity in the simulation domain, but the default 
is to set the resistivity to the constant value of \texttt{sim\_resistivity} 
everywhere.  For post processing, separate fields are stored for 
charge density and ion current density for species 1 and 2. 
\begin{table}[ht]
\caption{The grid variables for the hybrid solver that are most important 
         for a user. For each property for species 1, there is a corresponding 
         variable for species 2 (named with 2 instead of 1), 
         e.g., \texttt{cde2}.}
\begin{center}
\begin{tabular}{llp{18em}}
Variable & Type & Description \\
\hline 
\texttt{cden} & PER\_VOLUME & Total charge density, $\rho$ [C/m$^3$] \\
\texttt{grbx} & GENERIC & Magnetic field, $\mathbf{B}$ [T]  \\
\texttt{grby} & GENERIC &   \\
\texttt{grbz} & GENERIC &   \\
\texttt{grex} & GENERIC & Electric field, $\mathbf{E}$ [V/m]  \\
\texttt{grey} & GENERIC & \\
\texttt{grez} & GENERIC & \\
\texttt{grjx} & PER\_VOLUME & Current density, $\mathbf{J}$ [A/m$^2$]  \\
\texttt{grjy} & PER\_VOLUME &   \\
\texttt{grjz} & PER\_VOLUME &  \\
\texttt{gjix} & PER\_VOLUME & Ion current density, $\mathbf{J}_I$ [A/m$^2$]  \\
\texttt{gjiy} & PER\_VOLUME &   \\
\texttt{gjiz} & PER\_VOLUME &  \\
\texttt{cde1} & PER\_VOLUME & Species 1 charge density, $\rho_I$ [C/m$^3$] \\
\texttt{jix1} & PER\_VOLUME & Species 1 ion current density, $\mathbf{J}_I$ [A/m$^2$]  \\
\texttt{jiy1} & PER\_VOLUME &   \\
\texttt{jiz1} & PER\_VOLUME &  
\end{tabular}
\end{center}

\end{table}

Regarding particle properties. 
Each computational meta-particles is labeled by a positive integer, 
\texttt{specie} and has a \texttt{mass} and \texttt{charge}. 
As all Flash-X particles they also each have a 
position $\mathbf{r}_i$=(\texttt{posx}, \texttt{posy}, \texttt{posz}) 
and a velocity $\mathbf{v}_i$=(\texttt{velx}, \texttt{vely}, \texttt{velz}). 
To be able to deposit currents onto the grid, each particle stores 
the current density corresponding to the particle, $\mathbf{J}_{Ii}$. 
For the movement of the particles by the Lorentz force, we also need 
the electric and magnetic fields at the particle positions, 
$\mathbf{E}(\mathbf{r}_i)$ and $\mathbf{B}(\mathbf{r}_i)$, respectivly. 
\begin{table}[ht]
\caption{Important particle properties for the hybrid solver. 
         Note that this is for the computational macro-particles. }
\begin{center}
\begin{tabular}{llp{21em}}
Variable & Type & Description \\
\hline 
\texttt{specie} & REAL & Particle type (an integer number 1,2,3,\ldots)\\
\texttt{mass} & REAL & Mass of the particle, $m_i$ [kg] \\
\texttt{charge} & REAL & Charge of the particle, $q_i$ [C] \\
\texttt{jx} & REAL & Particle ion current, $\mathbf{J}_{Ii}=q_i\mathbf{v}_i$ [A\,m] \\
\texttt{jy} & REAL &  \\
\texttt{jz} & REAL &  \\
\texttt{bx} & REAL & Magnetic field at particle, $\mathbf{B}(\mathbf{r}_i)$ [T] \\
\texttt{by} & REAL &  \\
\texttt{bz} & REAL &  \\
\texttt{ex} & REAL & Electric field at particle, $\mathbf{E}(\mathbf{r}_i)$ [V/m] \\
\texttt{ey} & REAL &  \\
\texttt{ez} & REAL &
\end{tabular}
\end{center}

\end{table}

Regarding the choice of time step.  
The timestep must be constant, $\Delta t=$\texttt{dtinit = dtmin = dtmax}, 
since the leap frog solver requires this. 
For the solution to be stable 
the time step, $\Delta t$, must be small enough. 
We will try and quantify this, and here we asume that the grid 
cells are cubes, $\Delta x=\Delta y=\Delta z$, and that we have 
a single species plasma. 

First of all, since the time advancement is explicit, 
there is the standard CFL condition that no particle should travel 
more than one grid cell in one time step, i.e. 
$\Delta t \,\mbox{max}_i(|\mathbf{v}_i|) < \Delta x$. 
This time step is printed to standard output by Flash-X (as \texttt{dt\_Part}), 
and can thus be monitored. 

Secondly, we also have an upper limit on the time step due to 
Whistler waves (Pritchett 2000),
\[
  \Delta t < \frac{ \Omega_i^{-1} }{\pi}
    \left( \frac{\Delta x}{\delta_i} \right)^2
    \sim \frac{n}{B} \left(\Delta x\right)^2, 
  \qquad
  \delta_i = \frac{1}{|q_i|} \sqrt{\frac{m_i}{\mu_0\,n}}, 
\]
where $\delta_i$ is the ion inertial length, 
and $\Omega_i=|q_i|B/m_i$ is the ion gyrofrequency. 

Finally, we also want to resolve the ion gyro motion by having several 
time steps per gyration.  This will only put a limit on the time step 
if, approximately, $\Delta x > 5\,\delta_i$, and we then 
have that $\Delta t < \Omega_i^{-1}$. 

All this are only estimates that does not take into account, \eg,
the initial functions, or the sub\-cycling of the magnetic field update. 
In practice one can just reduce the time step until the solution is stable. 
Then for runs with different density and/or magnetic field strength, 
the time step will need to be scaled by the change in $n_{I}/B$, 
\eg, if $n_{I}$ is doubled, $\Delta t$ can be doubled. 
The factor $\left(\Delta x\right)^2$ implies that reducing the cell 
size by a factor of two will require a reduction in the time step by 
a factor of four. 



\subsection{Passive Particles}
\label{Sec:Passive Partices Integration}
Passive particles may be moved using one of several different methods
available in \flashx. With the exception of \textbf{Midpoint},
they are all single-step schemes. The methods are either first-order
or second-order accurate, and all are explicit, as described below.
In all implementations,
particle velocities are obtained by mapping grid-based velocities onto
particle positions as described in \secref{Sec:Particles Mapping}.

Numerically solving Equation~\eqref{Eqn:particle_motion_velocity} for passive particles
means solving a set of simple ODE initial value problems, separately for each particle,
where the velocities ${\bf v}_i$ are given at certain discrete points in time
by the state of the hydrodynamic velocity field at those times.
The time step is thus externally given and cannot be arbitrarily chosen by
a particle motion ODE solver\footnote{Even though it is possible to do so, see \api{Particles/Particles_computeDt}, one does not in general wish to let particles integration dictate the time step of the simulation.}. Statements about the order of a method in this context should
be understood as referring to the same method if it were applied in a hypothetical
simulation where evaluations of velocities ${\bf v}_i$ could be performed at
arbitrary times (and with unlimited accuracy). Note that \flashx does not
attempt to provide a particle motion ODE solver of higher accuracy than second order,
since it makes little sense to integrate particle motion to a higher order
than the fluid dynamics that provide its inputs.

In all cases, particles are advanced in time from $t^n$ (or, in the
case of \code{Midpoint}, from $t^{n-1}$) to $t^{n+1} = t^n + \Delta
t^n$ using one of the difference equations described below. The
implementations assume that at the time when
\api{Particles/Particles_advance} is called, the fluid fields have
already been updated to $t^{n+1}$, as is the case with the
\api{Driver/Driver_evolveFlash} implementations provided with
\flashx. A specific implementation of the passive portion of
\api{Particles/Particles_advance} 
 is selected by a setup option such
as \code{-with-unit=\-Particles/\-ParticlesMain/\-passive/\-Euler}, or by
specifying something like \code{REQUIRES
Particles/\-ParticlesMain/passive/\-Euler} in a simulation's
\code{Config} file (or by listing the path in the \code{Units} file if
not using the \code{-auto} configuration option).  
Further details are given is \secref{Sec:ParticlesUsing} below.


\begin{itemize}
\item {\bf Forward Euler (\code{Particles/\-ParticlesMain/passive/\-Euler}).}
Particles are
      advanced in time from $t^n$ to $t^{n+1} = t^n + \Delta t^n$
      using the following difference equation:
      \begin{equation}
    \label{Eqn:particle_passive_euler}
      {\bf x}_i^{n+1} = {\bf x}_i^n + {\bf v}_i^n\Delta t^n \quad.
      \end{equation}
Here ${\bf v}_i^n$ is the velocity of the particle, which is obtained using particle-mesh interpolation from the grid
at $t=t^n$.

Note that this evaluation of ${\bf v}_i^n$ cannot be deferred until the time when it is
needed at $t=t^{n+1}$, since at that point the fluid variables have been updated
and the velocity fields at $t=t^n$ are not available any more. Particle velocities are therefore
interpolated from the mesh at $t=t^n$ and stored as particle attributes.
Similar concerns apply to the remaining methods but will not be explicitly mentioned
every time.



\item{\textbf{Two-Stage Runge-Kutta (\code{Particles/\-ParticlesMain/passive/\-RungeKutta}).}}
This 2-stage Runge-Kutta scheme is the preferred choice in \flashx. It is also the default which
is compiled in if particles are included in the setup but no specific \childunit is requested.
The scheme is also known as Heun's Method:
      \begin{eqnarray}
    \label{Eqn:particle_passive_rk2}
      {\bf x}_i^{n+1} &=& {\bf x}_i^n + \frac{\Delta t^n}{2} 
                     \left[ {\bf v}_i^n + {\bf v}_i^{*,n+1} \right]      \;,\\
      \hbox{where}\quad {\bf v}_i^{*,n+1} &=& {\bf v}({\bf x}_i^{*,n+1},t^{n+1})    \;,\nonumber\\
      {\bf x}_i^{*,n+1} &=& {\bf x}_i^n + \Delta t^n
                     {\bf v}_i^n       \quad.\nonumber
      \end{eqnarray}
Here ${\bf v}({\bf x},t)$ denotes evaluation (interpolation) of the fluid velocity field
at position $\bf x$ and time $t$;
      ${\bf v}_i^{*,n+1}$ and ${\bf x}_i^{*,n+1}$ are intermediate
results \footnote{They can be considered ``predicted'' positions and velocities.};
and
${\bf v}_i^n = {\bf v}({\bf x}_i^{n},t^{n})$ is the velocity of the particle, obtained using particle-mesh interpolation from the grid
at $t=t^n$ as usual.


\item{\textbf{Midpoint (\code{Particles/\-ParticlesMain/passive/\-Midpoint}).}}
This Midpoint scheme is a two-step scheme.  Here, the particles are advanced from time
 $t^{n-1}$ to $t^{n+1} = t^{n-1} + \Delta t^{n-1} + \Delta t^{n}$ by the equation
     \begin{equation}
      {\bf x}_i^{n+1} = {\bf x}_i^{n-1} + {\bf v}_i^{n}(\Delta t^{n-1} + \Delta t^{n}) \quad.
      \end{equation}
The scheme is second order if $\Delta t^n = \Delta t^{n-1}$.

To get the scheme started, an Euler step (as described for \code{passive/Euler}) is taken the first time 
\code{Particles/\-ParticlesMain/passive/\-Midpoint/\-pt_advancePassive} is called.

The \code{Midpoint} \childunit uses the following additional particle attributes:
\begin{codeseg}
PARTICLEPROP pos2PrevX REAL           # two previous x-coordinate
PARTICLEPROP pos2PrevY REAL           # two previous y-coordinate
PARTICLEPROP pos2PrevZ REAL           # two previous z-coordinate
\end{codeseg}


% \item{\textbf{Estimated Midpoint (\code{Particles/\-ParticlesMain/\-EstiMidpointPassive}).}}
% This is a corrected version of what used to be known as Predictor-Corrector in \flashx and pre-release
% distributions of the \flashx code.
% The \emterm{Estimated Midpoint} scheme is second order when time steps remain constant.  
% Particle advancement follows the equation
% \newcommand{\XP}{\textbf{xp(t+1.5*dtNew)}}
% \newcommand{\VP}{\textbf{vp(t+1.5*dtNew)}}
% \newcommand{\Deltat}{{\Delta t}}
% \catcode`\_=8
% \newcommand{\xn}{{\bf x}_i^{n}}
% \newcommand{\vn}{{\bf v}_i^{n}}
% \newcommand{\tn}{t^{n}}
% \newcommand{\tp}{t_{*}^{n+\frac12}}
% \newcommand{\tP}{t^n + \Deltatp }
% \newcommand{\thalf}{t^{n+\frac12}}
% \newcommand{\tH}{t^n + \frac12 \Delta t^{n} }
% %\newcommand{\Deltatp}{\Delta t^{*,n-\frac12} }
% \newcommand{\Deltatp}{\Delta t_{*}^{n} }
% \newcommand{\DeltatP}{\frac12 \Delta t^{n-1} }
% \newcommand{\Deltathalf}{\frac12 \Delta t^{n}}
% \newcommand{\xh}{{\bf x}_i^{{\mathrm{E}},n+\frac12}}
% \newcommand{\xp}{{\bf x}_i^{*,n+\frac12}}
% \newcommand{\vp}{{\bf v}_i^{*,n+\frac12}}
% %begin{latexonly}
% \makeUsNormal
% %end{latexonly}
%      \begin{eqnarray}
%     \label{Eqn:EstiMidpoint}
%       {\bf x}_i^{n+1} &=& {\bf x}_i^n + 
%   \frac{\Delta t^n}{2}  \left[ {\bf v}_i^{*,n+\frac{1}{2}} + {\bf v}(\xp,t^{n+1}) \right]      \;,\\
%       \hbox{where}\quad \vp &=& {\bf v}(\xp,t^{n})    \;,\nonumber\\
%       \xp &=& {\bf x}_i^n + \frac12 \Delta t^{n-1} {\bf v}_i^n  \quad.\nonumber
%       \end{eqnarray}
% Here, ${\bf v}_i^{*,n+\frac{1}{2}}$ is a predicted velocity, evaluated at $t^n$ at a point $\xp$
% estimated (at $t^n$) to be the midpoint that will be reached halfway between $t^n$ and $t^{n+1}$.
% The term ${\bf v}(\xp,t^{n+1})$ represents another evaluation at the same position but
% at time $t^{n+1}$. The velocity at the midpoint at time $t + \frac{\Delta t}{2}$, which is not
% directly available for evaluation, is thus estimated as the average of two evaluations at
% the two closest times that are available. 
% 
% Note that the timestep that is used to compute the estimated midpoint position
% $\xp$ is not $\frac12\Deltat^n$ (which is
% exactly half of the length of the interval $[t^n,t^{n+1}]$ and thus should be used),
% but it is $\frac12\Deltat^{n-1}$, half of the lenght of the previous time interval
% $[t^{n-1},t^n]$ \footnote{The provided implementations of
% \api{Driver/Driver_evolveFlash} call 
% \api{Driver/Driver_computeDt} to compute the next time step $\Deltat^{n+1}$ only after
% all physics units, including \unit{Particles}, are done with advancing the solution
% to $t^{n+1}$. A reason for this is that the new time step may depend on the updated
% solution (including, in principle, the state of particles). Thus $\Deltat^{n+1}$
% is not available in the \code{Particles_advance} call that advances particles
% from $t^n$ to $t^{n+1}$ and which has to compute ${\bf x}_i^{*,n+\frac32}$
% and ${\bf v}_i^{*,n+\frac32} = {\bf v}({\bf x}_i^{*,n+\frac32},t^{n+1})$
% for use in the next iteration.}.
% When the time step changes, the actual midpoint time will deviate from
% the one used in the estimate.
% 
% 
% The scheme is second order if $\Delta t^n = \Delta t^{n+1}$.
% When timesteps $\Delta t$ are changing,
% the accuracy of the method is less than second order.
% An Euler step, as described for \code{Euler} in \eqref{Eqn:particle_passive_euler}, 
%  is taken the first time when
% \code{Particles/ParticlesMain/EstiMidpoint2Passive/Particles\_advance} is called and whenever the
% time step has changed too much. The timestep is considered to have changed too much
% if the following is true:
% \begin{displaymath}
%     \left| \Delta t^{n} - \Delta t^{n-1} \right| \geq
%                                                       \code{pt\_dtChangeTolerance} \times  \Delta t^{n-1},
% \end{displaymath}
% where \rpi{Particles/pt\_dtChangeTolerance} is a runtime parameter.
% 
% 
% The \code{EstiMidpoint} \childunit uses the following additional particle attributes
% for storing the values of $\xp$ and $\vp$ between the \code{Particles_advance} calls at $t^n$
% and $t^{n+1}$:
% \begin{codeseg}
% PARTICLEPROP velPredX REAL
% PARTICLEPROP velPredY REAL
% PARTICLEPROP velPredZ REAL
% PARTICLEPROP posPredX REAL
% PARTICLEPROP posPredY REAL
% PARTICLEPROP posPredZ REAL
% \end{codeseg}
% 

\item{\textbf{Estimated Midpoint with Correction (\code{Particles/\-ParticlesMain/passive/EstiMidpoint2}).}}
The scheme is second order even if $\Delta t^n = \Delta t^{n+1}$ is not assumed.
It is essentially the \code{EstiMidpoint} or ``Predictor-Corrector'' method of
previous releases, with a correction for 
non-constant time steps
by using additional evaluations (at times and positions
that are easily available, without requiring more particle attributes).

Particle advancement follows the equation
%begin{latexonly}
\catcode`\_=8\relax
%end{latexonly}
% This crap from previously in the file which was commented out
% Klaus, could you define the damn terms consistently at the TOP of the file?
\newcommand{\XP}{\textbf{xp(t+1.5*dtNew)}}
\newcommand{\VP}{\textbf{vp(t+1.5*dtNew)}}
\newcommand{\Deltat}{{\Delta t}}
\catcode`\_=8
\newcommand{\xn}{{\bf x}_i^{n}}
\newcommand{\vn}{{\bf v}_i^{n}}
\newcommand{\tn}{t^{n}}
\newcommand{\tp}{t_{*}^{n+\frac12}}
\newcommand{\tP}{t^n + \Deltatp }
\newcommand{\thalf}{t^{n+\frac12}}
\newcommand{\tH}{t^n + \frac12 \Delta t^{n} }
%\newcommand{\Deltatp}{\Delta t^{*,n-\frac12} }
\newcommand{\Deltatp}{\Delta t_{*}^{n} }
\newcommand{\DeltatP}{\frac12 \Delta t^{n-1} }
\newcommand{\Deltathalf}{\frac12 \Delta t^{n}}
\newcommand{\xh}{{\bf x}_i^{{\mathrm{E}},n+\frac12}}
\newcommand{\xp}{{\bf x}_i^{*,n+\frac12}}
\newcommand{\vp}{{\bf v}_i^{*,n+\frac12}}
% These nearly identical things were defined for this section.  Right.
\newcommand{\vcomb}{{\bf v}_i^{\mathrm{comb}}}
\newcommand{\CA}{c_1(\Deltat^{n-1},\Deltat^n)}
\newcommand{\CB}{c_2(\Deltat^{n-1},\Deltat^n)}
\newcommand{\CC}{c_3(\Deltat^{n-1},\Deltat^n)}
\newcommand{\CD}{c_4(\Deltat^{n-1},\Deltat^n)}
\newcommand{\cA}{c_1}
\newcommand{\cB}{c_2}
\newcommand{\cC}{c_3}
\newcommand{\cD}{c_4}
%\newcommand{\e}{{\mathbf{e}}_i}

\begin{equation}
\label{Eqn:EstiMidpoint2}
{\bf x}_i^{n+1} = {\bf x}_i^n +   {\Deltat}^n \, \vcomb      \;,
\end{equation}
where
\begin{equation}
\label{Eqn:pt_em2-2}
\vcomb =
\cA\,{\bf v}(\xp,t^{n})   +
\cB\,{\bf v}(\xp,t^{n+1})   +
\cC\,{\bf v}(\xn,t^{n})   +
\cD\,{\bf v}(\xn,t^{n+1})
\end{equation}
is a combination of four evaluations (two each at the previous and the current time),
\begin{equation*}
\xp = {\bf x}_i^n + \frac12 \Delta t^{n-1} {\bf v}_i^n
\end{equation*}
are estimated midpoint positions as before in the Estimated Midpoint scheme,
and the coefficients
\begin{eqnarray*}
\cA&=&\CA\;,\\
\cB&=&\CB\;,\\
\cC&=&\CC\;,\\
\cD&=&\CD
\end{eqnarray*}
are chosen dependent on the change in time step so that the method stays
second order when $\Deltat^{n-1} \neq \Deltat^n$.


Conditions for the correction can be derived as follows:
Let 
$\Deltatp=\DeltatP$  the estimated half time step
used in the scheme, let $\tp=\tP$ the estimated midpoint time,
and $\thalf=\tH$ the actual midpoint of the $[t^n,t^{n+1}]$ interval.
Also write $\xh = \xn+ \Deltathalf\vn$ for 
first-order (Euler) approximate positions
at the actual midpoint time $\thalf$,
and we continue to denote with $\xp$ the estimated positions 
reached at the estimated mipoint time $\tp$.

Assuming reasonably smooth functions ${\bf v}({\bf x},t)$,
we can then write 
 for the exact value of the velocity field at the approximate
positions
evaluated at the actual midpoint time
\begin{equation}
    \label{Eqn:vhalf_expansion}
{\bf v}(\xh,\thalf) = 
   {\bf v}(\xn,\tn) +
      {\bf v}_t(\xn,\tn)\Deltathalf +
      (\vn \cdot \frac{\partial}{\partial {\bf x}}) {\bf v}(\xn,\tn)\Deltathalf +
         O((\Deltathalf)^2)
\end{equation}
by Taylor expansion. 
It is known that the propagation scheme
$\widetilde{{\bf x}}_i^{n+1}=\xn+{\bf v}(\xh,\thalf) \Deltat$ 
using these velocities is second order
(this is known as the modified Euler method).


On the other hand, expansion of \eqref{Eqn:pt_em2-2} gives
\begin{eqnarray}
\vcomb &=& 
(\cA+\cB+\cC+\cD) {\bf v}(\xn,\tn) \nonumber\\
&& +\;
   (\cB +    \cD) {\bf v}_t(\xn,\tn) \Deltat \;+\;
  (\cA+\cB )  (\vn \cdot \frac{\partial}{\partial {\bf x}}) {\bf v}(\xn,\tn) \Deltatp  \nonumber\\
&& \quad +    \quad    \mbox{higher order terms in $\Deltat$ and $\Deltatp$.} \nonumber
\end{eqnarray}
After introducing a  time step factor $f$ defined by
$\Deltatp = f\,\Deltat^{n}$, this becomes
\begin{eqnarray}
    \label{Eqn:vcomb_expansion}
\vcomb &=& 
(\cA+\cB+\cC+\cD) {\bf v}(\xn,\tn) \\
&& +\;
   (\cB +    \cD) {\bf v}_t(\xn,\tn) \Deltat \;+\;
  (\cA+\cB )(\vn \cdot \frac{\partial}{\partial {\bf x}}) {\bf v}(\xn,\tn) \,f\,\Deltat  \nonumber\\
&& \quad  +  \; O((\Deltat)^2)\quad. \nonumber
\end{eqnarray}


One can derive conditions for second order accuracy by comparing
\eqref{Eqn:vcomb_expansion} with \eqref{Eqn:vhalf_expansion} and requiring that
\begin{equation}
\vcomb =  {\bf v}(\xh,\thalf) +  O((\Deltat)^2)\quad.
\end{equation}
It turns out that the coefficients have to satisfy three conditions
in order to eliminate from the theoretical difference between 
numerical and exact solution
all $O(\Deltat^{n-1})$ and $O(\Deltat^{n})$ error terms:
\begin{eqnarray*}
   \cA+\cB+\cC+\cD &=& 1  \quad  \mbox{(otherwise the scheme will not even be of first order)}\;,\\
      \cB   +\cD &=& \frac12 \quad  \mbox{(and thus also $\cA+\cC=\frac12$)}\;, \\ 
   \cA+\cB       &=& \frac{ \Deltat^n } { \Deltat^{n-1} } \quad.
\end{eqnarray*}
The provided implementation chooses $\cD=0$ (this can be easily changed if desired by editing in the code).
All four coefficients are then determined:
     \begin{eqnarray*}
\cA&=&\frac{ \Deltat^n } { \Deltat^{n-1} } - \frac12\;,\\
\cB&=&\frac12\;,\\
\cC&=&1 - \frac{ \Deltat^n } { \Deltat^{n-1} }\;,\\
\cD&=&0\quad.
      \end{eqnarray*}
Note that when the time step remains unchanged we have $\cA=\cB=\frac12$ and $\cC=\cD=0$,
and so \eqref{Eqn:EstiMidpoint2} simplifies significantly.
% to \eqref{Eqn:EstiMidpoint}.



An Euler step, as described for \code{passive/Euler} in \eqref{Eqn:particle_passive_euler}, 
 is taken the first time when
\code{Particles/\-ParticlesMain/passive/\-EstiMidpoint2/\-pt\_advancePassive} is called and when the
time step has changed too much. 
Since the integration scheme is tolerant of time step changes, it should
usually not be necessary to apply the second criterion; even when it is to be employed,
the criteria should be less strict than for an uncorrected \code{EstiMidpoint} scheme.
For \code{EstiMidPoint2} the timestep is considered to have changed too much
if either of the following is true:
\begin{displaymath}
     \Delta t^{n} > \Delta t^{n-1} 
\quad\mbox{and}\quad
    \left| \Delta t^{n} - \Delta t^{n-1} \right| \geq
                                                      \code{pt\_dtChangeToleranceUp} \times  \Delta t^{n-1}
\end{displaymath}
or 
\begin{displaymath}
     \Delta t^{n} < \Delta t^{n-1} 
\quad\mbox{and}\quad
    \left| \Delta t^{n} - \Delta t^{n-1} \right| \geq
                                                      \code{pt\_dtChangeToleranceDown} \times  \Delta t^{n-1},
\end{displaymath}
where \rpi{Particles/pt\_dtChangeToleranceUp} and \rpi{Particles/pt\_dtChangeToleranceDown} are runtime parameter
specific to the \code{EstiMidPoint2} \childunit.



The \code{EstiMidpoint2} \childunit uses the following additional particle attributes
for storing the values of $\xp$ and $\vp$ between the \code{Particles\_advance} calls at $t^n$
and $t^{n+1}$:
\begin{codeseg}
PARTICLEPROP velPredX REAL
PARTICLEPROP velPredY REAL
PARTICLEPROP velPredZ REAL
PARTICLEPROP posPredX REAL
PARTICLEPROP posPredY REAL
PARTICLEPROP posPredZ REAL
\end{codeseg}

\end{itemize}


The time integration of passive particles is tested in the \code{ParticlesAdvance} unit test,
which can be used to examine the convergence behavior, see \secref{Sec:ParticlesUnitTest}.


%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------

\section{Mesh/Particle Mapping}
\label{Sec:Particles Mapping}

Particles behave in a fundamentally different way than grid-based quantities.
Lagrangian, or passive particles are essentially independent of the grid mesh and move
along with the velocity field.  Active particles may be located independently of mesh refinement.
In either case, there is a need to convert grid-based quantities into similar attributes defined
on particles, or vice versa.
The method for interpolating mesh quantities to tracer particle positions must
be consistent with the numerical method to avoid introducing systematic
error. In the case of a finite-volume methods such as those used in \flashx, the mesh
quantities have cell-averaged rather than point values, which requires
that the interpolation function for the particles also represent
cell-averaged values. Cell averaged quantities are defined as
\begin{equation}
f_i\left(x\right) \: \equiv \: \frac{1}{\Delta x} \int_{x_{i-1/2}}^{x_{i-1/2}}
f\left(x^\prime \right) dx^\prime
\end{equation}
where $i$ is the cell index and $\Delta x$ is the spatial
resolution.
The mapping back and forth from the mesh to the particle properties
are defined in the routines \code{Particles_\-mapFromMesh} and
\code{Particles_\-mapToMeshOneBlk}.

Specifying the desired mapping method is accomplished by designating the \code{MAPMETHOD}
in the Simulation \code{Config} file for each type of particle.
See \secref{Sec:FlashHparttypes} for more details.



\subsection{Quadratic Mesh Mapping}

The quadratic mapping package defines an interpolation back and forth
to the mesh which is second order. This implementation is primarily
meant to be used with passive tracer particles.

To derive it, first consider a
second-order interpolation function of the form
\begin{equation}
f\left(x\right) \: = \: A + B \left(x - x_i\right) + C \left(x - x_i\right)^2 \; .
\end{equation}
Then integrating gives
\begin{eqnarray}
f_{i-1} \: & = & \: \frac{1}{\Delta x}\left[ A + \left. \frac{1}{2}
B \left(x - x_i\right)^2 \right|^{x_{i-1/2}}_{x_{i-3/2}} + \left. \frac{1}{3}
C \left(x - x_i\right)^3 \right|^{x_{i-1/2}}_{x_{i-3/2}} \right]\nonumber  \\
           & = & \: A - B\Delta x + \frac{13}{12} C \Delta x^2 \;,
\end{eqnarray}
\begin{eqnarray}
f_{i} \: & = & \: \frac{1}{\Delta x}\left[ A + \left. \frac{1}{2} B \left(x - x_i\right)^2 \right|^{x_{i+1/2}}_{x_{i-1/2}}
+ \left. \frac{1}{3}  C \left(x - x_i\right)^3 \right|^{x_{i+1/2}}_{x_{i-1/2}}  \right]\nonumber \\
           & = & \: A + \frac{1}{12} C \Delta x^2 \; ,
\end{eqnarray}
and
\begin{eqnarray}
f_{i+1} \: & = & \: \frac{1}{\Delta x}\left[ A + \left.\frac{1}{2} B \left(x - x_i\right)^2 \right|^{x_{i+3/2}}_{x_{i+1/2}}.
+ \left. \frac{1}{3}  C \left(x - x_i\right)^3 \right|^{x_{i-1/2}}_{x_{i-3/2}} \right]\nonumber  \\
           & = & \: A - B\Delta x + \frac{13}{12} C \Delta x^2 \;,
\end{eqnarray}
We may write these as
\begin{equation}
\left[ \begin{array}{c}
f_{i+1} \\ f_{i} \\ f_{i-1}
\end{array} \right] \: = \:
\left[ \begin{array}{ccc}
1 & -1 & \frac{13}{12} \\ 1 & 0 & \frac{1}{12} \\ 1 & 1 & \frac{13}{12}
\end{array} \right]
\left[ \begin{array}{ccc}
A \\ B\Delta x \\ C \Delta x^2
\end{array} \right] \; .
\end{equation}
Inverting this gives expressions for $A$, $B$, and $C$,
\begin{equation}
\left[ \begin{array}{c}
A \\ B\Delta x \\ C \Delta x^2
\end{array} \right] \: = \:
\left[ \begin{array}{ccc}
-\frac{1}{24} & \frac{13}{12} & -\frac{1}{24} \\ -\frac{1}{2} & 0 & \frac{1}{2} \\
\frac{1}{2} & -1 & \frac{1}{2}
\end{array} \right]
\left[ \begin{array}{ccc}
f_{i+1} \\ f_{i} \\ f_{i-1}
\end{array} \right] \; .
\end{equation}

In two dimensions, we  want a second-order interpolation function of the form
\begin{equation}
f\left(x,y\right) \: = \: A + B \left(x - x_i\right) + C \left(x - x_i\right)^2
+ D \left(y - y_j\right) + E \left(y - y_j\right)^2 +
F \left(x - x_i\right)\left(y - y_j\right) \; .
\end{equation}
In this case, the cell averaged quantities are given by
\begin{equation}
f_{i,j}\left(x,y\right) \: \equiv \: \frac{1}{\Delta y}{\Delta x} \int_{x_{i-1/2}}^{x_{i+1/2}}
dx^\prime
\int_{y_{j-1/2}}^{x_{j-1/2}}  dy^\prime
f\left(x^\prime,y^\prime \right) \; .
\end{equation}
Integrating the 9 possible cell averages gives, after some algebra,
\begin{equation}
\left[ \begin{array}{c}
f_{i-1,j-1} \\ f_{i,j-1} \\ f_{i+1,j-1} \\
f_{i-1,j} \\ f_{i,j} \\ f_{i+1,j} \\
f_{i-1,j+1} \\ f_{i,j+1} \\ f_{i+1,j+1}
\end{array} \right] \: = \:
\left[ \begin{array}{cccccc}
1 & -1 & \frac{13}{12} & -1 &  \frac{13}{12} &  1 \\
1 &  0 & \frac{1}{12}  & -1 &  \frac{13}{12} &  0 \\
1 &  1 & \frac{13}{12} & -1 &  \frac{13}{12} & -1 \\
1 & -1 & \frac{13}{12} &  0 &  \frac{1}{12}  &  0 \\
1 &  0 & \frac{1}{12}  &  0 &  \frac{1}{12}  &  0 \\
1 &  1 & \frac{13}{12} &  0 &  \frac{1}{12}  &  0 \\
1 & -1 & \frac{13}{12} &  1 &  \frac{13}{12} & -1 \\
1 &  0 & \frac{1}{12}  &  1 &  \frac{13}{12} &  0 \\
1 &  1 & \frac{13}{12} &  1 &  \frac{13}{12} &  1
\end{array} \right]
\left[ \begin{array}{c}
A \\ B\Delta x \\ C \Delta x^2 \\
D \Delta y \\ E \Delta y^2 \\ F \Delta x \Delta y
\end{array} \right] \; .
\end{equation}
At this point we note that there are more constraints than unknowns,
and we must make a choice of the constraints. We chose to ignore
the cross terms and take only the face-centered cells next to
the cell containing the particle, giving
\begin{equation}
\left[ \begin{array}{c}
f_{i,j-1} \\ f_{i-1,j} \\ f_{i,j} \\
f_{i+1,j} \\ f_{i,j+1}
\end{array} \right] \: = \:
\left[ \begin{array}{cccccc}
1 &  0 & \frac{1}{12}  & -1 &  \frac{13}{12}  \\
1 & -1 & \frac{13}{12} &  0 &  \frac{1}{12}   \\
1 &  0 & \frac{1}{12}  &  0 &  \frac{1}{12}   \\
1 &  1 & \frac{13}{12} &  0 &  \frac{1}{12}   \\
1 &  0 & \frac{1}{12}  &  1 &  \frac{13}{12}
\end{array} \right]
\left[ \begin{array}{c}
A \\ B\Delta x \\ C \Delta x^2 \\
D \Delta y \\ E \Delta y^2
\end{array} \right] \; .
\end{equation}
Inverting gives
\begin{equation}
\left[ \begin{array}{c}
A \\ B\Delta x \\ C \Delta x^2 \\
D \Delta y \\ E \Delta y^2
\end{array} \right]
\: = \:
\left[ \begin{array}{cccccc}
-\frac{1}{24} & -\frac{1}{24}  & \frac{7}{6}  & -\frac{1}{24}  & -\frac{1}{24}  \\
0             & -\frac{1}{2}   & 0            &  \frac{1}{2}   &  0   \\
0             & \frac{1}{2}    & -1           &  \frac{1}{2}   &  0   \\
-\frac{1}{2}  &  0             & 0            &  0             &  \frac{1}{2}   \\
\frac{1}{2}    &  0             & -1           &  0             &  \frac{1}{2}
\end{array} \right]
\left[ \begin{array}{c}
f_{i,j-1} \\ f_{i-1,j} \\ f_{i,j} \\
f_{i+1,j} \\ f_{i,j+1}
\end{array} \right]  \; .
\end{equation}
Similarly, in three dimensions, the interpolation function is
\begin{equation}
f\left(x,y,z\right) \: = \: A + B \left(x - x_i\right) + C \left(x - x_i\right)^2
+ D \left(y - y_j\right) + E \left(y - y_j\right)^2 +
F \left(z - z_k\right) + G  \left(z - z_k\right)^2 \; .
\end{equation}
and we have
\begin{equation}
\left[ \begin{array}{c}
A \\ B \Delta x \\ C {\Delta x}^2 \\
D \Delta y \\ E {\Delta y}^2 \\ F \Delta z \\ G {\Delta z}^2
\end{array} \right]
\: = \:
\left[ \begin{array}{ccccccc}
-\frac{1}{24} & -\frac{1}{24} & -\frac{1}{24} & \frac{5}{4} & -\frac{1}{24} & -\frac{1}{24} & -\frac{1}{24} \\
0             & 0             & -\frac{1}{2}  & 0           &  \frac{1}{2}  & 0             &  0   \\
0             & 0             &  \frac{1}{2}  & -1          &  \frac{1}{2}  & 0             &  0   \\
0             & -\frac{1}{2}  &  0            &  0          &  0            & \frac{1}{2}   &  0   \\
0             &  \frac{1}{2}  &  0            & -1          &  0            & \frac{1}{2}   &  0   \\
-\frac{1}{2}  & 0             &  0            &  0          &  0            & 0             &  \frac{1}{2}   \\
 \frac{1}{2}  & 0             &  0            & -1          &  0            & 0             &  \frac{1}{2}
\end{array} \right]
\left[ \begin{array}{c}
f_{i,j,k-1} \\ f_{i,j-1,k} \\ f_{-i,j,k} \\
f_{i,j,k} \\ f_{i+1,j,k} \\ f_{i,j+1,k} \\ f_{i,j,k+1}
\end{array} \right]  \; .
\end{equation}
Finally, the above expressions apply only to Cartesian coordinates. In the case of
cylindrical $\left(r,z\right)$ coordinates, we have
\begin{eqnarray}
f\left(r,z\right) \; = & & \nonumber \\
& A + B \left(r - r_i\right) + C \left(r - r_i\right)^2
+ D \left(z - z_j\right) & \nonumber \\
& + E \left(z - z_j\right)^2 +
F \left(r - r_i\right)\left(z - z_j\right) \; . &
\end{eqnarray}
and
\begin{eqnarray}
\left[ \begin{array}{c}
A \\ B\Delta r \\ C\Delta r^{\frac{2}{6}} \\ D\Delta z  \\ E \Delta z^2
\end{array} \right] \: = \hspace{-1.0in} & & \nonumber \\
& \left[ \begin{array}{cccccc}
-\frac{1}{24} & -\frac{h_1-1}{24h_1}  & \frac{7}{6}      & -\frac{h_1-1}{24h1}  & -\frac{1}{24} \\
0
& - \frac{\left(7+6h_1\right)\left(h_1-1\right)}{3h_2}
& \frac{2h_1}{3h2}
& \frac{\left(7+6h_1\right)\left(h_1-1\right)}{3h_2}
& 0  \\
0
& \frac{\left(12h_1^2 + 12 h_1 - 1\right)\left(h_1 - 1\right)}{h_1h_2}
& -2 \frac{12 h_1^2 -13}{h_2}
& - \frac{\left(12h_1^2 + 12 h_1 - 1\right)\left(h_1 - 1\right)}{h_1h_2}
& 0 \\
- \frac{1}{2}
& 0
& 0
& \frac{1}{2}
& 0 \\
0
& \frac{1}{2}
& - 1
& \frac{1}{2}
& 0 \\
\end{array} \right]
\left[ \begin{array}{c}
f_{i,j-1} \\ f_{i-1,j} \\ f_{i,j} \\
f_{i+1,j} \\ f_{i,j+1}
\end{array} \right]  \; . &
\end{eqnarray}


\subsection{Cloud in Cell Mapping}
\label{Sec:CIC}
Other interpolation routines can be defined that take into account
the actual quantities defined on the grid.  These ``mesh-based''
algorithms are represented in \flashx by the Cloud-in-Cell
mapping, where the interpolation to/from the particles is defined
as a simple linear weighting from nearby grid points.  The weights are
defined by considering only the region of one ``cell'' size around
each particle location; the proportional volume of the particle
``cloud'' corresponds to the amount allocated to/from the mesh. The
\code{CIC} method can be used with both types of particles. When using it
with active particles the MapToMesh methods should also be
selected. In order to include the CIC method with passive particles,
the \code{setup} command line option is
\code{-with-unit=Particles/ParticlesMapping/CIC}. Two additional
command line option
\code{-with-unit=Particles/ParticlesMapping/MapToMesh} and
\code{-with-unit=Grid/GridParticles/MapToMesh} are necessary 
when using the active particles. All of these command line options can
be replaced by placing the appropriate \code{REQUIRES/REQUESTS}
directives in the Simulation \code{Config} file.

\section{Using the Particles Unit}
\label{Sec:ParticlesUsing}
The Particles unit encompasses nearly all aspects of Lagrangian
particles.  The exceptions are input/output
the movement of related data structures
between different blocks as the particles move from one block to
another, and 
mapping the particle attributes to and from the grid. 

Beginning with release of version 4 it is possible to add particles to
a simulation during evolution, a new function \code{Particles\_addNew}
has been added to the unit's API for this purpose.
It has been possible to include  
multiple different types of particles in the same simulation since
release \flashx. Particle 
types must be specified in the \code{Config} file of the Simulations
unit setup directory for the application, and the syntax is explained
in \secref{Sec:FlashHparticles}.
At configuration time, the setup script parses the \code{PARTICLETYPE}
specifications in the Config files, and generates an F90 file 
\api{Particles/Particles_specifyMethods}\code{.F90}
that
populates a data structure \code{gr_ptTypeInfo}. This data structure
contains information about the method of initialization and
interpolation methods for mapping the particle attributes to and from
the grid for each included particle type. Different time integration
schemes are applied to active and passive 
particles. However, in one simulation, all active particles are
integrated using the same 
scheme, regardless of how many active types exists. 
Similarly, only one passive integration scheme is used.
The added complexity of multiple particle types allows different methods
to be used for initialization of particles positions and their
mapping to and from the grid quantities.
Because several different implementations of each type of
functionality can co-exist in one simulation, there are no defaults in
the \code{Particles} unit \code{Config} files. These various functionalities are organized
into different \subunits; a brief description of each subunit is 
included below and further expanded in subsections in this chapter.  

\begin{itemize}

 \item The \code{ParticlesInitialization} subunit distributes a given
set of particles through the spatial domain at the simulation startup. Some
type of spatial initialization is always required; the functionality
is provided by \api{Particles/Particles_initPositions}. The users of active
particles typically have their own custom initialization. The
following two implementations of initialization techniques are
included in the \flashx distribution (they are more likely to
used with the passive tracer particles): 

      \begin{description}
      \item[\code{Lattice}] distributes particles regularly along the
	axes directions throughout a subsection of the physical grid.
      \item[\code{WithDensity}] distributes particles randomly, with
	particle density being proportional to the grid gas density.
      \end{description}

Users have two options for implementing custom initialization
methods. The two files involved in the process are:
\api{Particles/Particles_initPositions} and pt\_initPositions. The
former does some housekeeping such as allowing for inclusion
of one of the available methods along with the user specified one, and
assigning tags at the end. A user wishing to add one custom method
with no constraints on tags etc is advised to implement a custom
version of the latter. This approach allows the user to focus the
implementation on the placement of particles only. Users desirous of
refining the grid based on particles count during initialiation should
see the setup \code{PoisParticles} for an example implementation of
the Particles\_initPositions routine. If more than one implementation
of pt\_initPositions is desired in the same simulation then it is
necessary to implement each one separately with different names (as we
do for tracer particles: pt\_initPositionsLattice and
pt\_initPositionsWithDensity) in their simulation setup directory. In
addition, a modified copy of Particles\_initPostions, which calls
these new routines in the loop over types, must also be
placed in the same directory.

\item The \code{ParticlesMain} \subunit contains the various
  time-integration options for both active and passive particles.  A 
  detailed overview of the different schemes is given in
  \secref{Sec:Particles Integration}. 
%  %and  defines the solution variables listed in
%  % \tblref{Tab:longrangegravvariables}. Although the particle density (\code{PDEN_VAR}) is defined,
%  % note that it is not necessary to use the \code{Gravity/GravityMain/Poisson}
%  % implementation; externally imposed gravitational fields can also be used.
%   
%   \begin{comment}
%     \begin{table}[!ht]
%       
%       \caption{ \label{Tab:longrangegravvariables} Solution variables
% 	provided by the \code{Particles/ParticlesMain/active/longRange}
% 	
% 	\begin{center}
% 	  \begin{tabular}{lp{1.2in}lp{3in}}
% 	    Name           & Type           & Description\\
% 	    \hline
% 	    \code{PDEN_VAR}     & GENERIC   &
%             Mesh-mapped particle density\\
% 	    \code{grav}     & GENERIC   &
%             Gravitational acceleration
%             component\\
% 	    \hline
% 	  \end{tabular}
% 	\end{center}
% 	
%     \end{table}
%   \end{comment}


\item The \code{ParticlesMapping} \subunit controls the mapping of particle properties to and from the grid.
      Flash-X currently supplies the following mapping schemes:
      \begin{description}
%\begin{comment}
      %\item nearest grid point (\code{mapping/ngp}),
      \item[\code{Cloud-in-cell}] (\code{ParticlesMapping/\-meshWeighting/\-CIC}), which weights values at nearby grid cells;  and
%\end{comment}
        \item[\code{Quadratic}] (\code{ParticlesMapping/\-Quadratic}), which performs quadratic interpolation.
    %\item  triangular-shaped cloud  (\code{mapping/tsc}), and cloud-in-cell for 1D spherical and 2D
    %      axisymmetric cylindrical coordinates (\code{mapping/cic\_1dsph} and
    %  \item    \code{mapping/cic\_2dcylaxi}, respectively).
      \end{description}
      
      Some form of mapping must always be included when running a
simulation with particles. As mentioned in \secref{Sec:Particles
Mapping} the quadratic mapping scheme is only available to map {\em from} the
grid quantities to the corresponding particle attributes. Since active
particles require the same mapping scheme to be used in mapping to and
from the mesh, they cannot use the quadratic mapping scheme as
currently implemented in \flashx. The CIC scheme may be used by both
the active and passive particles. 

For active particles, we use the
mapping routines to assign particles' mass to the particle density
grid-based solution variable (\code{PDEN_VAR}).  This mapping is the initial step
in the particle-mesh (PM) technique for evaluating the long range
gravitational force between all particles.  Here, we use the particle
mapping routine \api{Particles/Particles_mapToMeshOneBlk} to ``smear'' the
particles' attribute over the cells of a temporary array.  The
temporary array is an input argument which is passed from the grid
mapping routine \api{Grid/Grid_mapParticlesToMesh}.  This encapsulation
means that the
particle mapping routine is independent of the current state of 
the grid, and is not tied to a particular \unit{Grid} implementation.  For
details about the task of mapping the temporary array values to the
cells of the appropriate block(s), please see
\secref{Sec:GridParticlesMapToMesh}. New schemes can be created that
``smear'' the particle across many more cells to give a more accurate
\code{PDEN_VAR} distribution, and thus a higher quality force
approximation between particles.  Any new scheme should implement a
customized version of the \code{pt_assignWeights} routine, so that it
can be used by the \code{Particles_mapToMeshOneBlk} routine during the
map. 
      
      

\item The \code{ParticlesForces} subunit implements the long and short range forces described in
  Equation~\eqref{Eqn:particle_motion_forces} in the following directories: 
  
  \begin{itemize}
  \item \code{longRange} collects different long-range force laws
    (requiring elliptic solvers or the like and dependent upon all other
    particles);
  \item \code{shortRange} collects different short-range force
    laws (directly summed or dependent upon nearest neighbors only).
  \end{itemize}
  
  Currently, only one long-range force law (gravitation) with one
  force method (particle-mesh) is included with Flash-X. Long-range
  force laws are contained in the \code{Particles/\-ParticlesForces/\-longRange},
  which requires that the \code{Gravity} unit be included in the code.
In the current release, no 
\code{shortRange} implementation of \code{ParticlesForces} is
supplied with
Flash-X. 
However, note that the sink particle implementation described below in
\secref{Sec:Particles Sink} includes
directly computed particle--particle forces.

\end{itemize}

After particles are moved during time integration or by forces, 
they may end up on other blocks within or
without the current processor.  The redistribution of particles
among processors is handled by the \unit{GridParticles} subunit, as the
algorithms required vary considerably between the grid
implementations.  The boundary conditions are also implemented by
the GridParticles unit.  See \secref{Sec:GridParticles} for more details of these
redistribution algorithms.  The user should include the option \code{-with-unit=Grid/\-GridParticles} on the
setup line, or \code{REQUIRES Grid/\-GridParticles} in the Config file.

In addition, the input-output routines for the Particles unit are contained in a
subunit \unit{IOParticles}.  Particles are written to the main checkpoint files.
If the user desires, a separate output file can be created which contains only the
particle information.  See \secref{Sec:Particles IO} below as well as
\secref{Sec:IOParticles} for more details.  The user should include the option
\code{-with-unit=IO/\-IOParticles} on the setup line, or \code{REQUIRES IO/\-IOParticles} in the Config file.

In \flashx, the initial particle positions can be used to
construct an appropriately refined grid, i.e. more refined in places
where there is a clustering of particles.  To use this feature the
\code{flash.par} file must include:
\code{refine_on_particle_count=.true.} and
\code{max_particles_per_blk=[some value]}.  Please be aware that Flash-X
will abort if the criterion is  too demanding.  To overcome the abort,
specify a less demanding criterion, or increase the value of
\code{lrefine_max}.  

\subsection{Particles Runtime Parameters}
\label{Sec:Particles Runtime Parameters}

There are several general runtime
parameters applicable to the \code{Particles} unit, which affect every implementation.
The variable \rpi{Particles/useParticles} obviously must be set equal to \code{.true.} to utilize
the Particles unit.  The time stepping is controlled with \rpi{Particles/pt_dtFactor};
a value less than one ensures that particles will not step farther than one entire cell in any
given  time interval.  The \code{Lattice} initialization routines have additional parameters.
The number of evenly spaced particles is controlled in each direction by
\rpi{Particles/pt_numX} and similar variables in $Y$ and $Z$.
The physical range of initialization is controlled by \rpi{Particles/pt_initialXMin}
and the like.
Finally, note that the output of particle properties to special particle files is controlled by
runtime parameters found in the \code{IO} unit.  See \secref{Sec:Particle files} for more details.

% \begin{comment}
% \tblref{Tab:particle_parameters} and indicated with the module notation "Main".
% Additional parameters are applicable only to the module indicated.
% 
% \begin{center}
% \begin{longtable}{llllp{3in}}
% \caption{ \label{Tab:particle_parameters} Runtime parameters used with
% the \code{Particles} unit} \\
% 
% 
% Variable                   & Module   & Type     & Default  & Description\\
% \hline
% \code{useParticles}               &Main & boolean  & True   & If true, evolve particles\\
% \code{pt_dtFactor}    &Main &  real     & 0.5 & Maximum distance (in cells) a
%                                              particle is allowed to travel in
%                                              one timestep. Should not be set
%                                              larger than the number of guard
%                                              cells.\\
% \code{pt_maxPerProc} &Main & integer  & 1000   & Maximum number of particles per
%                                              processor (sets size of particle
%                                              buffers)\\
% \code{pt_numX|Y|Z}       &Lattice & integer  & 1   & Sets the number of
%                                              particles along the $x,y,$ or $z$-dimension
%                                              of the physical grid\\
% 
% \code{pt_initialX|Y|Z|Min|Max}  & Lattice & real & 0.0 | 1.0 &
%                             Range in the physical domain where particles should be evenly
%                             distributed.\\
% \code{pt_numParticlesWanted}    &WithDensity & integer & 1 & Approximate number
%                             of particles desired throughout entire grid\\
% \code{pt_pRand}            &WithDensity & integer& 100000 & Random seed\\
% 
% code{particle_attribute_1|2..} & Main & string & &attributes that
% should be output \\
% \hline
% 
% \end{longtable}
% \end{center}
% \end{comment}



\subsection{Particle Attributes}
\label{Sec:Particle Properties}

By default, particles are defined to have eight real properties or attributes:  3 positions in x,y,z; 3 velocities
in x,y,z; the current block identification number; 
and a tag which uniquely identifies the particle.
Additional properties can be defined for each particle.  For example, active particles usually
have the additional properties of mass and acceleration (needed for the integration routines, see
Table \tblref{Tab:active particle attributes}).
Depending upon the simulation, the user can define particle properties
in a manner similar to that used
for mesh-based solution variables.
To define a particle attribute, add to
a \code{Config} file a line of the form
\begin{quote}
\code{PARTICLEPROP} {\it property-name}
\end{quote}

For attributes that are meant to merely sample and record the state of
certain mesh variables along trajectories, Flash-X can automatically
invoke interpolation (or, in general, some map method) to generate
attribute values from the appropriate grid quantities.
(For passive tracer particles, these are typically the only attributes
beyond the default set of eight mentioned above.)
The routine \api{Particles/Particles_updateAttributes} is invoked
by Flash-X at appropriate times to effect this mapping, namely before
writing particle data to checkpoint and particle plot files.
To direct the default implementation of \code{Particles\_updateAttributes}
to act as desired for tracer attributes, the user must define 
the association of the particle attribute
with the appropriate mesh variable by including the following line in
the \code{Config} file:
\begin{quote}
\code{PARTICLEMAP TO} {\it property-name} \code{FROM VARIABLE} {\it variable-name}
\end{quote}

These particle attributes are carried along in the simulation and
output in the checkpoint files. At runtime, the user can specify the attributes
to output through runtime parameters 
\newline
\rpi{Particles/particle_attribute_1},
\rpi{Particles/particle_attribute_2}, etc. These specified attributes
are collected in an array by the \api{Particles/Particles_init}
routine. This array in turn is used by
\api{Particles/Particles_updateAttributes} to calculate the values of the
specified attributes from the corresponding mesh quantities before
they are output.  

\subsection{Particle I/O}
\label{Sec:Particles IO}

Particle data are written to and read from checkpoint files by
the I/O modules (\chpref{Sec:Flash-X output formats}). For more
information on the format of particle data written to output files,
see \secref{Sec:HDF5} and \secref{Sec:PnetCDF IO}.

Particle data can also be written out to the \code{flash.dat} file.  The user should
include a local copy of \api{IO/IO_writeIntegralQuantities} in their Simulation directory.
The \code{Orbit} test problem supplies an example \code{IO_writeIntegralQuantities}
routine that is useful for writing individual
particle trajectories to disk at every timestep.

There is also a utility routine \api{Particles/Particles_dump} which can be used to dump
particle output to a plain text file.  An example of usage can be found in
 \api{Particles/Particles_unitTest}.  Output from this routine can be
read using the fidlr routine \code{particles_dump.pro}.

\subsection{Unit Tests}
\label{Sec:ParticlesUnitTest}
The unit tests provided for \unit{Particles} 
% exercise some basic functionality of the unit, in particular
% that particles stay assigned to the correct block and get reassigned to the correct block
% when they move outside of a block.
% Unit tests also 
exercise the \api{Particles/Particles_advance} methods for tracer particles.
Tests under
\code{Simulation/SimulationMain/unitTest/ParticlesAdvance} can be used to examine and compare convergence behavior of various time integration schemes.
The tests compare numerical and analytic solutions for a problem (with a given velocity field)
where analytic solutions can be computed.

Currently only one \code{ParticlesAdvance} test is provided. It is designed to be easily 
modified by replacing a few source files that contain implementations of the equation and the analytic solution.
The use the test, configure it with a command like
\begin{codeseg}
./setup -auto -1d unitTest/ParticlesAdvance/HomologousPassive \
  		-unit=Particles/ParticlesMain/passive/EstiMidpoint2
\end{codeseg}
and replace \code{EstiMidpoint2} with one of the other available methods (or omit the option to
get the default method), see \secref{Sec:Passive Partices Integration}. Add other options as desired.

For \code{unitTest/ParticlesAdvance/HomologousPassive},
\begin{codeseg}
./setup -auto -1d unitTest/ParticlesAdvance/HomologousPassive +ug -nxb=80
\end{codeseg}
is recommended to get started.

When varying the test, the following runtime parameters defined for
\newline
\code{Simulation/SimulationMain/unitTest/ParticlesAdvance} will probably need to
be adjusted:
\begin{description}
\item[\code{PARAMETER \rpi{Simulation/sim_schemeOrder} INTEGER 2} ---]The order of the integration scheme. This should probably always be
either 1 or 2.
\item[\code{PARAMETER \rpi{Simulation/sim_maxTolCoeff0} REAL 1.0e-8} ---]Zero-th order error coefficient $C_0$, used for convergence criterion if \code{sim_schemeOrder}$=0$.
\item[\code{PARAMETER \rpi{Simulation/sim_maxTolCoeff1} REAL 0.0001} ---]First order error coefficient $C_1$, used for convergence criterion if \code{sim_schemeOrder}$=1$.
\item[\code{PARAMETER \rpi{Simulation/sim_maxTolCoeff2} REAL 0.01} ---]Second order error coefficient $C_2$, used for convergence criterion if \code{sim_schemeOrder}$=2$.
\end{description}

A test for order $ k $ is considered successful if the following criterion is satisfied:
\begin{equation*}
\code{maxError} \leq C_k \times \code{maxActualDt}^k \;,
\end{equation*}
where \code{maxError} is the maximum absolute error between numerical and analytic solution for any particle
that was encountered during a simulation run, and \code{maxActualDt} is the maximum time step $\Delta t$ used
in the run. The appropriate runtime parameters of various units, in particular \unit{Driver}, \unit{Particles},
and \unit{Grid}, should be used to control the desired simulation run.
In particular, it is recommended to vary \rpi{Driver/dtmax} by several orders of magnitude 
(over a range where it directly determines \code{maxActualDt}) for a given test in 
order to examine convergence behavior.




%-------------------------------------------------------------------------------

%\begin{comment}
%LBR Note -- this is so simple for passive particles that I condensed it into
% the brief description above.  Active particles may need more info.
%\section{Initialization}
%\label{Sec:particles_initialization}
%
%The initial particle positions and velocities are set by the
%\code{InitParticlePositions()} routine. This routine accepts a
%single integer argument, the local ID number of the mesh block on
%which particles are to be initialized. For passive tracer particles,
%the mesh refinement pattern is not dependent upon particles, so
%generally one can assume that the most recent call to
%\code{mark\_grid\_refinement()} will have left the block refinement
%flags in an appropriate state. We initialize particles only on
%blocks that will not be refined again, are leaf nodes, and have not
%already been initialized. An example appears in the
%\code{InitParticlePositions()} routine supplied with the
%\code{particles/passive} module. This version of the routine seeds
%the grid with a uniform array of particles and sets their initial
%velocities by interpolation from the gas grid.
%
%% Active particles section
%Initial conditions for active particles are more complex. Here the
%refinement pattern may depend upon the particles, but we may wish to
%initialize more particles than can fit in the memory allocated to a
%single block (or a single processor). We need some means of
%predicting where particles will be, refining those regions, and then
%initializing particles only after the refined blocks have been set
%up. We do this by constructing a mesh-based ``guide function'' using
%a version of the \code{init\_block()} routine. Generally, this is
%some simple approximation of the mesh-mapped particle density field
%written to the particle density solution variable (\code{pden}). The
%AMR package is directed to refine on this variable. After the block
%refinement pattern has been set up, the particles are then
%initialized in the same way as passive particles, and a call to the
%mesh-mapping routine is performed to initialize the \code{pden}
%variable with the correct density field (so that it can be included
%in the initial checkpoint file). An example of this type of
%initialization is provided by the \code{orbit} test problem supplied
%with Flash-X.
%\end{comment}


%\begin{comment}
%
%\subsection{Active particles}
%\label{Sec:active particles}
%
%Active particles are not available in the Alpha release of \flashx.
%
%\subsubsection{Time Integration}
%
%
%\subsubsection{Forces}
%
%long range and short range.  Which are implemented?
%\end{comment}


%-------------------------------------------------------------------------------



%---------------------------------------------------------------------------------

%\begin{comment}
%\subsection{Active particles}
%
%-------------------------------------------------------------------------------




%\begin{comment}
%\section{Flash-X2 Predictor-Corrector in excruciating detail}
%As described above, the three-stage nuclear burning implemented for the
%deflagration simulations is relatively crude. In order to study the
%nuclear burning in more detail and calculate nucleosynthetic yields,
%we added the capability of including passive tracer particles in the
%simulations. These Lagrangian
%tracers record density and temperature histories for fluid elements during
%the course of a simulation and can then be post-processed with a detailed
%nuclear reaction network containing the many relevant species~\emph{brown+05}.
%In this section, we present details of our method of evolving tracer particles
%and verification tests of the method.
%
%The passive tracer particles evolve during the course of a simulation
%by a predictor-corrector time advancement scheme. Particle
%velocities are obtained by interpolation at the position of each
%particle at the beginning of each time step, and a predicted particle position
%after half a time step is calculated from the velocities. After evolution of
%the hydrodynamics, the velocities at the predicted positions are obtained
%by interpolation, and the particles then move according to the average of the
%the velocities at the two positions. This predictor-corrector scheme may
%be summarized as finding the average particle velocities at an imaginary
%time-centered particle location and using those average velocities to
%propagate the particles for one time step.
%
%The details of the particle propagation are as follows.
%\flashx makes use of a Strang-split computational cycle in which
%\begin{displaymath}
%\mathrm{1\;\;cycle} = 2 \Delta t =
%\left\{ \begin{array}{c}
%\Delta t \left\{ \mathrm{xyz\;\;sweep\/} \right. \\
%\Delta t \left\{ \mathrm{zyx\;\;sweep\/} \right.
%\end{array} \right. \\
%\end{displaymath}
%\begin{displaymath}
%\Delta t \rightarrow \Delta t^{\prime}  \mathrm{\;\;and\;\;repeat}
%\end{displaymath}
%Here we use the notation
%\begin{displaymath}
%\begin{array}{rl}
%\mathrm{particle\;velocity\;at}\;t = t^n: & v^n_p \\
%\mathrm{particle\;position\;at}\;t = t^n: & x^n_p \\
%\mathrm{hydrodynamics\;velocity\;at}\;t = t^n: & v^n \\
%\mathrm{interpolation\;``function"}\;: &v^n_p = v_p^n\left(x^m_p\right) =
%v_p\left(v^n,x^m_p\right) \;\;,
%\end{array}
%\end{displaymath}
%and note that $n$ counts increments of $\Delta t$. The process of interpolation
%will be described below.
%
%A computation cycle begins with the evolution of the hydrodynamics
%for one time step ($\Delta t$). For each particle, the predictor step calculates the
%velocity at its present position, calculates a predicted position from that
%velocity, and then calculates the velocity of that particle at that
%predicted position. We refer to this set of computations as
%``block A'' for convenience, and may write it as
%\begin{displaymath}
%\mathrm{block\;A}
%\left\{ \begin{array}{c}
%v_p^n\left(x_p^n\right) = v_p\left(v^n,x_p^n\right)  \\
%x_p^{n+\frac{1}{2}} = x_p^n + v_p^n\left(x_p^n\right)\frac{\Delta t}{2} \\
%%v_p^n\left(x_p^{n+\frac{1}{2}}\right) = v_p\left(v^n,x_p^{n+\frac{1}{2}}\right)
%v_p^n(x_p^{n+\frac{1}{2}}) = v_p(v^n,x_p^{n+\frac{1}{2}})
%\end{array} \right. \\
%\end{displaymath}
%
%The corrector step calculates the particle position from the average of the predicted velocity
%and an evolved velocity after one hydrodynamic step.
%
%The computation cycle continues after the predictor step with a hydrodynamic evolution
%for $\Delta t$.  After this evolution, the corrector step begins by calculating new
%velocities of the particles at the predicted positions found by interpolation of the new
%hydrodynamic velocity. The corrector step then evolves the particles for $\Delta t$ to using
%the average of the predicted and new velocities of each particle. We refer to this set
%of computations as ``block B'', and may write it as
%\begin{displaymath}
%\mathrm{block\;B}
%\left\{ \begin{array}{rl}
%v_p^{n+1}(x_p^{n+\frac{1}{2}}) &=
%v_p(v^{n+1},x_p^{n+\frac{1}{2}})  \\
%v_p^{n+\frac{1}{2}}(x_p^{n+\frac{1}{2}}) &=
%\frac{1}{2} \{v_p^{n+1}(x_p^{n+\frac{1}{2}}) +
%v_p^{n}(x_p^{n+\frac{1}{2}}) \} \\
%x_p^{n+1} =& x_p^n + v_p^{n+\frac{1}{2}}(x_p^{n+\frac{1}{2}}) \Delta t
%\end{array} \right. \\
%\end{displaymath}
%After this evolution, the particles are in new positions at
%$t = t^n + \Delta t$, and the process repeats to evolve the particles
%to the end of the computation cycle ($t = t^n + 2 \Delta t$) and calculate new
%predicted velocities ($t = t^n + 2 \Delta t + 0.5\Delta t$).
%Pseudo code for the evolution of one computational cycle:
%\begin{verbatim}
%
%if (firstcall)              ! get the velocity at the predicted
%  call blockA               ! position from initial conditions
%end if
%
%call hydro_3d(xyz)
%
%
%call advancePart:
%
%         if (not firstcall)
%
%           call blockB      ! update particle positions
%                            ! here t + dt
%         end if
%
%         call blockA        ! get velocity at new predicted position
%                            ! here t + dt + 0.5 dt
%
%call hydro_3d(zyx)
%
%call advancePart:
%
%       if (not firstcall)
%
%         call blockB        ! update particle positions
%                            ! here t + 2.0 dt
%       end if
%
%       call blockA          ! get velocity at new predicted position
%                            ! here t + 2.0 dt + 0.5 dt
%
%\end{verbatim}
%
%\end{comment}  % end of Flash-X2 predictor-corrector in excruciating detail.


% ======== START Sinks (C. Federrath) =========

%%\newpage

% \section{Sink Particles}
% \label{Sec:Particles Sink}

% \subsection{Basics of Sink Particles}

% Sink particles are required in collapse simulations to model dense core, star, or black hole formation and accretion. Using sink particles solves two main problems in collapse calculations:
% \begin{enumerate}
% \item The physical length scale associated with the collapse, the Jeans length, 
% \begin{equation} \label{eq:JeansLength}
% \lambda_\mathrm{J}=\sqrt{\frac{\pi c_\mathrm{s}^2}{G\rho}}
% \end{equation}
% decreases with increasing gas density $\rho$ (here, $G$ is the gravitational constant and $c_\mathrm{s}$ the sound speed). To avoid artificial fragmentation, the Jeans length must be resolved with at least 4 grid cells (Truelove et al.~1997, ApJ 489, L179). More recent calculations show that a minimum of 32 grid cells is required to resolve the kinetic energy in rotational motions inside the Jeans volume and to resolve minimum turbulent MHD dynamo action (Sur et al.~2010, ApJ 721, L134; Federrath et al.~2011, ApJ 731, 62). The Flash-X sink particle module automatically refines the AMR grid based on the Jeans length (controlled by runtime parameters \code{refineOnJeansLength}, \code{jeans\_ncells\_ref}, and \code{jeans\_ncells\_deref}). However, once the highest level of the AMR hierarchy is reached, sink particles must be created to avoid artificial fragmentation when the Jeans length drops further during the collapse.
% \item The typical timescale for collapsing objects is the freefall time, $t_\mathrm{ff}=\sqrt{3\pi/(32G\rho)}$. With increasing gas density, this timescale becomes shorter and shorter, which means that the code will literally grind to a halt during runaway collapse, as time steps become shorter and shorter. The first object going into freefall collapse thus determines the time step of the whole calculation, such that following the formation of a star cluster would be impossible. To avoid these problems, the gas in regions that are in a state of runaway collapse are replaced by sink particles.
% \end{enumerate}



% \subsection{Using the Sink Particle Unit}

% To include sink particles,
% specify \code{REQUIRES Particles/\-ParticlesMain/\-active/\-Sink}
% in the simulation
% \code{Config} file. This automatically includes all required \subunits
% for the sink particles. Currently, sink particles require a
% Paramesh~4 \unit{Grid} implementation and Poisson gravity. Currently, various \subunits
% and implementation directories of the \unit{Particles} unit and are also included
% automatically. Table~\ref{tab:sinks} lists all runtime parameters of
% the sink particle module. The default values of these parameters
% should be ok in most simulations, except \code{sink\_density\_thresh},
% \code{sink\_accretion\_radius}, and
% \code{sink\_softening\_radius}. Those three parameters have to be
% adopted to the resolution and physics of a given simulation.

% First, the sink particle accretion radius, $r_\mathrm{acc}$ (\code{sink\_accretion\_radius}) should be calculated, based on the minimum cell size $\Delta x$ for a given simulation. The recommended value is $r_\mathrm{acc}=2.5\Delta x$. If sink particles are included, the code will print a message to the standard output, stating the number of cells that a chosen sink accretion radius corresponds to. The recommended value for the sink particle softening radius (\code{sink\_softening\_radius}) is to set it equal to the accretion radius.

% In order to avoid artificial fragmentation, the gas density on the AMR grid must not exceed a critical density $\rho_\mathrm{thresh}$ in regions of gravitational collapse (Truelove et al.~1997). This density is related to the smallest resolvable Jeans length $\lambda_\mathrm{J}$ on the highest level of the AMR hierarchy. The density threshold $\rho_\mathrm{thresh}$ (\code{sink\_density\_thresh}) is obtained by solving the definition of the Jeans length, Equation~(\ref{eq:JeansLength}) for $\rho$,
% \begin{equation} \label{eq:rhothresh}
% \rho_\mathrm{thresh} = \frac{\pi c_\mathrm{s}^2}{G \lambda_\mathrm{J}^2} = \frac{\pi c_\mathrm{s}^2}{4 G r_\mathrm{acc}^2}\;.
% \end{equation}
% This equation relates the sink particle accretion radius to the sink particle density threshold and depends on the thermodynamics (sound speed) of a given simulation. Since the Jeans length should be resolved with at least 4 grid cells (Truelove et al.~1997), the sink particle accretion radius $r_\mathrm{acc}$ must not be smaller than 2 grid cells. The accretion radius is thus determined by the smallest linear cell size $\Delta x$ of the AMR grid. As recommended above, setting $r_\mathrm{acc}\simeq2.5\Delta x$ satisfies the Truelove criterion, because the Jeans length $\lambda_\mathrm{J}=2 r_\mathrm{acc}$ is thus resolved with 5 grid cells on the top level of the AMR hierarchy.

% Please cite Federrath et al.~(2010, ApJ 713, 269) if you are using this unit.


% \begin{table}[ht]
% \caption{Runtime parameters for the sink particle module.} \label{tab:sinks}
% \begin{center}
% \begin{tabular}{lllp{14em}}
% Variable & Type & Default & Description \\
% \hline 
% \texttt{useSinkParticles} & BOOLEAN & .false. & switch sinks on/off \\
% \texttt{sink\_density\_thresh} & REAL & 1.0e-14 & density threshold for sink creation and accretion \\
% \texttt{sink\_accretion\_radius} & REAL & 1.0e14 & creation and accretion radius \\
% \texttt{sink\_softening\_radius} & REAL & 1.0e14 & gravitational softening radius \\
% \texttt{sink\_softening\_type\_gas} & STRING & "linear" & sink--gas softening type (options: "linear", "spline") \\
% \texttt{sink\_softening\_type\_sinks} & STRING & "spline" & sink--sink softening type (options: "linear", "spline") \\
% \texttt{sink\_integrator} & STRING & "leapfrog" & sink particle time integrator (options: "euler", "leapfrog", "leapfrog\_cosmo") \\
% \texttt{sink\_dt\_factor} & REAL & 0.5 & time step safety factor ($\leq0.5$) \\
% \texttt{sink\_subdt\_factor} & REAL & 0.01 & time step safety factor for sink--sink subcycling ($\leq0.5$) \\
% \texttt{sink\_convergingFlowCheck} & BOOLEAN & .true. & creation check for converging gas flow \\
% \texttt{sink\_potentialMinCheck} & BOOLEAN & .true. & creation check for gravitational potential minimum \\
% \texttt{sink\_jeansCheck} & BOOLEAN & .true. & creation check for Jeans instability \\
% \texttt{sink\_negativeEtotCheck} & BOOLEAN & .true. & creation check for gravitationally bound gas \\
% \texttt{sink\_GasAccretionChecks} & BOOLEAN & .true. & check for bound and converging state before gas accretion \\
% \texttt{sink\_merging} & BOOLEAN & .false. & switch for sink particle merging \\
% \texttt{sink\_offDomainSupport} & BOOLEAN & .false. & support for sink particles to remain active when leaving the grid domain (in case of outflow boundary conditions) \\
% \texttt{sink\_AdvanceSerialComputation} & BOOLEAN & .true. & use the global sink particle array for time advancement (to greatly speed up computation of sink--sink interaction) \\
% \texttt{pt\_maxSinksPerProc} & INTEGER & 100 & number of sinks per processor \\
% \texttt{refineOnSinkParticles} & BOOLEAN & .true. & sinks must be on highest AMR level \\
% \texttt{refineOnJeansLength} & BOOLEAN & .true. & switch for refinement on Jeans length \\
% \texttt{jeans\_ncells\_ref} & REAL & 32.0 & number of cells for Jeans length refinement \\
% \texttt{jeans\_ncells\_deref} & REAL & 64.0 & number of cells for Jeans length de-refinement \\
% \end{tabular}
% \end{center}
% \end{table}


% \subsection{The Sink Particle Method}

% For technical details and tests of the Flash-X sink particle unit, see Federrath et al.~(2010, ApJ 713, 269) and Federrath et al.~(2011, IAUS 270, 425). We only summarize here the most important aspects of the implementation.

% Before a sink particle is created, we define a spherical control volume with the accretion radius $r_\mathrm{acc}$ around each cell that exceeds the resolution-dependent density threshold, $\rho_\mathrm{thresh}$ given by Equation~(\ref{eq:rhothresh}), and check whether the gas in this control volume
% \begin{itemize}
% \item is on the highest level of refinement,
% \item is converging ($\nabla\cdot\mathbf{v} < 0$; i.e., has negative radial velocity),
% \item has a central gravitational potential minimum,
% \item is gravitationally bound ($|E_\mathrm{grav}|>E_\mathrm{int}+E_\mathrm{kin}+E_\mathrm{mag}$),
% \item is gravitationally unstable (Jeans-unstable),
% \item and is not within the accretion radius of an existing sink particle.
% \end{itemize}
% These checks are designed to avoid spurious sink particle formation and to trace only truly collapsing and star-forming gas. As soon as a sink particle is created, it can gain further mass by accretion from the AMR grid, if this gas is inside the sink particle accretion radius and is bound and collapsing towards the sink particle. If the accretion radii of multiple sink particles overlap, the gas of a given cell is accreted onto the sink particle to which the gas is most strongly bound.

% We take all contributions to the gravitational interactions between the gas on the grid and the sink particles into account. Those interactions are
% \begin{enumerate}
% \item GAS--GAS (handled by either the multigrid or the tree Poisson solver)
% \item GAS--SINKS (computed by direct summation over all particles and grid cells)
% \item SINKS--GAS (computed by direct summation over all particles and grid cells)
% \item SINKS--SINKS (computed by direct N-body summation over all sink particles).
% \end{enumerate}
% We note that all interactions including sink particles (GAS--SINKS, SINKS--GAS, SINKS--SINKS) do not require the Poisson solver or any particle--grid mapping procedure. Since these are all computed by direct summation, computational costs can become quite expensive once the number of sink particles reaches thousands and more, but is significantly more accurate than mapping procedures. In fact, a previous implementation used interpolation of sink particle masses back onto the grid and employing the Poisson solver to compute all interactions. This resulted in very smooth gravitational interactions that made it impossible to follow close orbits and highly eccentric encounters of multiple (two and more) sink particles, in particular for the sink--sink interactions. Thus, the current implementation of sink particles is designed to follow a maximum of a few thousand particles. Linear and spline softening of the gravitational forces for extremely close encounters of multiple sink particles are implemented. By default, linear softening is used for gas--sinks and sinks--gas interactions (\code{sink\_softening\_type\_gas}), motivated by an almost uniform gas density inside the sink particle radius. Spline softening is used for sink--sink interactions (\code{sink\_softening\_type\_sinks}), as it is more suitable to follow N-body dynamics. Softening is only applied inside the sink softening radius (\code{sink\_softening\_radius}). A second-order accurate leapfrog integrator is used to advance the sink particles. For cosmological simulations, a cosmological version of the leapfrog integrator is available (see \code{sink\_integrator}). The sink particles are fully integrated into the MPI parallelization of the Flash-X code. Any split or unsplit solver for hydro or MHD (in which case the bound state check includes the contribution of the magnetic energy) is supported.


% \subsection{Sink Particle Unit Test}

% To invoke the sink particle unit test, use \code{./setup unitTest/SinkMomTest -auto -3d}. This initializes a momentum conservation test. An initially uniform cloud with radius $0.025\,\mathrm{pc}$ and density $10^{-18}\,\mathrm{g}\,\mathrm{cm}^{-3}$ at rest starts collapsing and forms a sink particle. An initial sink particle with $0.1\,M_\odot$ and initial momentum $p_y=4\times10^{36}\,\mathrm{g}\,\mathrm{cm}\,\mathrm{s}^{-1}$ in positive $y$-direction is also present. The purpose of this test is to see how well the total $x$-momentum $p_x$ and the total $y$-momentum $p_y$ are conserved. This test is particularly suitable, because it involves gas collapse, sink particle creation, accretion, more than one sink particle, and thus all gravitational interactions (gas--gas, gas--sinks, sinks--gas, sinks--sinks), as well as close eccentric orbits of the two sink particles. Figure~\ref{fig:sinkmomentumtest} shows $p_x$ and $p_y$ as a function of time for both sink particles and gas separately. The symmetry between sink and gas momenta shows that momentum is well conserved for several orbits. Figure~\ref{fig:sinkmomentumtest} can be reproduced with the IDL and Python tools in \code{/source/Simulation/SimulationMain/unitTest/SinkMomTest/utils/} by running the IDL script \code{plot_mom_sinks.pro}.

% \begin{figure}
% \centerline{
% \includegraphics[width=0.5\linewidth]{sinks_momentum_x}
% \includegraphics[width=0.5\linewidth]{sinks_momentum_y}
% }
% \caption{Sink particle unit test to check momentum conservation during sink particle creation, accretion, and eccentric orbits.}
% \label{fig:sinkmomentumtest}
% \end{figure}


\clearpage

% ======== END Sinks (C. Federrath) =========
