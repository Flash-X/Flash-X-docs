\chapter{Driver Unit}
\label{Chp:Driver Unit}

The \unit{Driver} unit controls the initialization and evolution of Flash-X simulations.
In addition, at the highest level, the \unit{Driver} unit organizes the interaction between units. Initialization can be from scratch or
from a stored checkpoint file. For advancing the solution, the drivers
can use either an operator-splitting technique adapted to directionally 
split physics operators like split \unit{Hydro}  (\code{Split}), or a
more generic ``\code{Unsplit}'' implementation.
%%The unsplit operator is
%%currently specific to the problems that use the unsplit
%%\code{StaggeredMesh} MHD solver. 
The \unit{Driver} unit also calls the
\unit{IO} unit at the end of every timestep to produce checkpoint
files, plot files, or other output.

\section{Driver Routines} \label{Driver Routines}
The most important routines in the \unit{Driver} API are those that initialize,
evolve, and finalize the Flash-X program.
The file
\code{Flash.F90}
contains the main Flash-X program (equivalent to \code{main()} in C).
The default top-level program of Flash-X,
\code{Simulation/Flash.F90}, calls \unit{Driver} routines in this order:

\begin{codeseg}
program Flash

  implicit none
  
  call Driver_initParallel()
  call Driver_initFlash()
  call Driver_evolveFlash( )
  call Driver_finalizeFlash ( )

end program Flash
\end{codeseg}


Therefore the no-operation stubs for these routines in the \code{Driver} source directory
must be overridden by an implementation function in a \uid under the \unit{Driver}
or \code{Simulation} directory trees,
in order for
a simulation to perform any meaningful actions.
The most commonly used implementations
for most of these files are located in the
\code{Driver/DriverMain} \uid, with
a few specialized ones in either
\code{Driver/DriverMain/Split} or
\code{Driver/DriverMain/Unsplit}.



\subsection{\code{Driver\_initFlash}}
The first of these routines is \api{Driver/Driver\_initParallel}, which
  initializes the parallel environment for the simulation. New in
  \flashx is an ability to replicate the mesh where more than one 
copy of the discretized mesh may exist with some overlapping and some 
non-overlapping variables. Because of this feature, the 
parallel environment differentiates between global and mesh
communicators. All the necessary communicators, and the attendant
meta-data is generated in this routine. Also because of this
modification, runtime parameters such as iProcs, jProcs etc, which
were under the control of the Grid unit in \flashx, are now under
the control of the Driver unit. Several new accessor interface
allow other code units to query the driver unit for this information.
The \api{Driver/Driver\_initFlash}, the next routine, in general calls the
initialization routines in each of the units.  If a unit is not
included in a simulation, its stub (or empty) implementation is
called.  Having stub implementations is very useful in 
the \unit{Driver} unit because it allows the user to avoid writing a new driver
for each simulation.  For a more detailed explanation of stub implementations please see \secref{Sec:Unit Architecture}.
It is important to note that when individual units are being
initialized, order is often very important and the order of initialization is different depending
on whether the run is from scratch or being restarted from a checkpoint file.

\subsection{\code{Driver\_evolveFlash}}
The next routine is \api{Driver/Driver\_evolveFlash} which controls
the timestepping of the simulation, as well as the normal termination of Flash-X
based on time.  \code{Driver\_evolveFlash} checks the parameters \code{tmax},
\code{nend} and \code{zFinal} to determine that the run should end, having 
reached a particular point in time, a certain number of steps, or a particular
cosmological redshift, respectively.  Likewise the initial simulation time, 
step number and cosmological redshift for a simulation can be set using
the runtime parameters \code{tmin}, \code{nbegin}, and \code{zInitial}. 
This version of Flash-X includes versions of \code{Driver\_evolveFlash} for 
directionally-split and unsplit staggered mesh operators.

\subsubsection{Strang Split Evolution}
The code in the \code{Driver/DriverMain/Split} \uid has been the default
time update method earlier, and can still be be used for
many setups that can be configured with
Flash-X.  The routine  \code{Driver\_evolveFlash} implements a
Strang-split method of time advancement where each physics unit
updates the solution data for two equal timesteps -- thus the sequence
of calls to physics and other units in each time step goes something like this: 
\unit{Hydro}, diffusive terms, source terms, \unit{Particles}, \unit{Gravity};
\unit{Hydro}, diffusive terms, source terms, \unit{Particles}, \unit{Gravity}, 
\unit{IO} (for output), \unit{Grid} (for grid changes).
The
hydrodynamics update routines take a ``sweep order'' argument since
they must be directionally split to work with this driver. Here, the
first call usually uses the ordering $x-y-z$, and the second call uses
$z-y-x$. Each of the update routines is assumed to directly modify the
solution variables. At the end of one loop of timestep advancement,
the condition for updating the mesh refinement pattern is tested if
the adaptive mesh is being used, and a refinement update is carried
out if required.

\subsubsection{Unsplit Evolution}
The driver implementation in the \code{Driver/DriverMain/Unsplit} directory is
the default since \flashx. 
It is required
specifically for the two unsplit solvers:
unsplit staggered mesh MHD solver (\secref{Sec:usm_algorithm}) and the unsplit 
gas hydrodynamics solver (\secref{Sec:unsplit hydro algorithm}).
This implementation in general calls each of the physics routines only once 
per time step, and
each call advances solution vectors by one timestep.
At the end of one loop of timestep advancement, the condition for updating the 
adaptive mesh refinement pattern is tested and applied.


\subsubsection{Super-Time-Stepping (STS)}
A new timestepping method implemented in \flashx is a technique called
Super-Time-Stepping (STS). The STS is a simple explicit method which is
used to accelerate restrictive parabolic timestepping advancements ($\Delta t_{\mbox{CFL\_para}}\approx \Delta x^2$)
by relaxing the CFL stability condition of parabolic equation system.

The STS has been proposed by Alexiades et al., (1996), and used in computational astrophysics and sciences recenly
(see Mignone et al., 2007; O'Sullivan \& Downes, 2006; Commer\c{c}on et al., 2011; Lee, D. et al, 2011)
for solving systems of parabolic PDEs numerically.  The method increases its effective time steps $\Delta t_{\mbox{sts}}$
using two properties of stability and optimality in
Chebychev polynomial of degree $n$.  These properties optimally
maximize the time step $\Delta t_{\mbox{sts}}$ by which a solution vector can be
evolved. A stability condition is imposed only after each time 
step $\Delta t_{\mbox{sts}}$, which is further subdivided into smaller $N_{\mbox{sts}}$
sub-time steps, $\tau_i$,\; that is, $\Delta t_{\mbox{sts}}=\sum^{N_{\mbox{sts}}}_{i=1} \tau_i$, where
the sub-time step is given by
\begin{equation}
\tau_i=\Delta t_{\mbox{CFL\_para}}\big[ (-1+\nu_{\mbox{sts}}) cos\big(\frac{\pi(2j-1)}{2N_{\mbox{sts}}}  + 1+\nu_{\mbox{sts}}  \big)\big]^{-1},
\label{eqn:STS_original}
\end{equation}
where $\Delta t_{\mbox{CFL\_para}}$ is an explicit time step for a given parabolic system based on the CFL stability
condition. $\nu$ (\code{nuSTS}) is a free parameter less than unity. For $\nu \rightarrow 0$, STS is asymptotically
$N_{\mbox{sts}}$ times faster than the conventional explicit scheme based on the CFL condition.
During the $N_{sts}$ sub-time steps, the STS method still solves 
solutions at each intermediate step $\tau_i$; however, 
such solutions should not be considered as meaningful solutions.

Extended from the original STS method for accelerating parabolic timestepping, 
our STS method advances advection and/or diffusion (hyperbolic and/or parabolic) system of equations. 
This means that the STS algorithm in Flash-X invokes
a single $\Delta t_{sts}$ for both advection and diffusion,
and does not use any sub-cycling for diffusion based on a given advection time step.  
In this case, $\tau_i$ is given by
\begin{equation}
\tau_i=\Delta t_{\mbox{CFL}}\big[ (-1+\nu_{\mbox{sts}}) cos\big(\frac{\pi(2j-1)}{2N_{\mbox{sts}}}  + 1+\nu_{\mbox{sts}} \big)\big]^{-1},
\label{eqn:STS_flash}
\end{equation}
where $\Delta t_{\mbox{CFL}}$ can be an explicit time step for advection ($\Delta t_{\mbox{CFL\_adv}}$) 
or parabolic ($\Delta t_{\mbox{CFL\_para}}$) systems.
%In practice, the former will accelerate $\Delta t_{\mbox{CFL\_adv}}$ by a modest amount ($\sim 10-15$\%) in solving
%hyperbolic systems, and the latter 
%In many advection-diffusion simulations when the grid is progressively refined, a severe restriction in timestepping
%takes over. It is often the case that we have $\Delta t_{\mbox{CFL\_adv}} \gg \Delta t_{\mbox{CFL\_para}}$.
%In Flash-X's STS implementation can choose to 
In cases of advection-diffusion system, $\Delta t_{\mbox{CFL}}$ takes ($\Delta t_{\mbox{CFL\_para}}$) when it is smaller
than ($\Delta t_{\mbox{CFL\_adv}}$); otherwise, Flash-X's timestepping will proceed without using STS iterations (i.e.,
using standard explicit timestepping that is either Strang split evolution or unsplit evolution.


Since the method is explicit, it works equally well
 on both a uniform grid and AMR grids without modification. 
The STS method is first-order accurate in time.

Both directionally-split and unsplit hydro solvers can use the STS method,
simply by invoking a runtime parameter \code{useSTS = .true.} in \code{flash.par}.
There are couple of runtime parameters that control solution accuracy and stability.
They are decribed in Table {\ref{Tab:sts parameters}}.

\begin{table}
\caption{ Runtime parameters for STS}
\label{Tab:sts parameters}
\begin{center}
\begin{tabular}{lllp{3.5in}}
Variable                   & Type    & Default       & Description\\
\hline
\code{useSTS}              & logical & .false.       & Enable STS \\
\code{nstepTotalSTS}       & integer &  5            & Suggestion: $\sim$ 5 for hyperbolic; $\sim$ 10 for parabolic\\
\code{nuSTS}               & real    &  0.2          & Suggestion: $\sim$ 0.2 for hyperbolic; $\sim$ 0.01 for parabolic. It is known that a very low value of $\nu$ may result in unstable temporal integrations, while a value close to unity can decrease the expected efficiency of STS.\\
\code{useSTSforDiffusion}  & logical & .false.       & This setup will use the STS for overcoming small diffusion time steps assuming $\Delta t_{\mbox{CFL\_adv}} > \Delta t_{\mbox{CFL\_para}}$. In implementation, it will set $\Delta t_{\mbox{CFL}}=\Delta t_{\mbox{CFL\_para}}$ in Eqn. {\ref{eqn:STS_flash}}. Do not allow to turn on this switch when there is no diffusion (viscosity, conductivity, and magnetic resistivity) used. \\
\code{allowDtSTSDominate}  & logical & .false.       & If true, this will allow to have $\tau_i > \Delta t_{\mbox{CFL\_adv}}$, which may result in unstable integrations.\\
\hline
\end{tabular}
\end{center}
\end{table}

%Users are recommended to see further references 


\subsubsection{Runtime Parameters}
The \unit{Driver} unit supplies certain runtime parameters regardless of
which type of driver is chosen. These are described in the
online \rpi*{Driver/Runtime Parameters Documentation page}.

\begin{flashtip}
The \unit{Driver} unit no longer provides runtime parameters,
physical constants, or logfile management.  Those services have been
placed in separate units.  The \unit{Driver} unit also does not declare
boolean values to include a unit in a simulation or not.  For example,
in \flashx, the \unit{Driver} declared a runtime parameter \code{iburn} to turn on and off
burning.
\begin{codeseg}
if(iburn) then
    call burning ....
end if
\end{codeseg}
 In \flashx the individual unit declares a runtime parameter that
 determines whether the unit is used during the simulation \eg, the \unit{Burn} unit declares
\code{useBurn} within the \unit{Burn} unit code that
turns burning on or off.  This way the \unit{Driver} is no longer responsible for knowing what is included in a simulation.
A unit gets called from the \unit{Driver}, and if it is not included in a simulation, a stub gets called.
If a unit, like \unit{Burn}, is included but the user wants to turn burning off, then the runtime parameter
declared in the \unit{Burn} unit would be set to false.
\end{flashtip}


\subsection{\code{Driver\_finalizeFlash}}
Finally, the the \unit{Driver} unit calls \api{Driver/Driver_finalizeFlash} which calls the finalize routines for each unit.  Typically this involves deallocating memory and any other necessary cleanup.


\subsection{Driver accessor functions}
In \flashx the \code{Driver} unit also provides a number of
accessor functions to get data stored in the \code{Driver} unit, for
example \api{Driver/Driver_getDt},  \api{Driver/Driver_getNStep},  
 \api{Driver/Driver_getElapsedWCTime},  \api{Driver/Driver_getSimTime}.

\begin{flashtip}
In \flashx most of the quantities that were in the \flashx database are stored in the
\code{Grid} unit or are replaced with functionality in the \code{Flash.h} file.  A few scalars
quantities like \code{dt}, the current timestep number \code{nstep}, simulation time and elapsed
wall clock time, however, are now stored in the \code{Driver\_data} FORTRAN90 module.
\end{flashtip}

The \code{Driver} unit API also defines two interfaces for halting the code,
\api{Driver/Driver_abortFlash} and
\newline % prevent overfull
\capi{Driver/Driver_abortFlashC}\code{.c}.
The '\code{c}' routine
version is available for calls written in C, so that the user does not have to worry about
any name mangling.  Both of these routines print an error message and
call \code{MPI\_Abort}.


\subsection{Time Step Limiting}
The \unit{Driver} unit is responsible for determining the time step $\Delta t$
that is used to advance the solution from time $t=t^{n-1}$ to $t^n$.
At startup, a tentative $\Delta t$ is chosen based in runtime parameters,
in particular \rpi{Driver/dtinit}.
The routine \api{Driver/Driver_verifyInitDt} (usually invoked from \api{Driver/Driver_initFlash}) is used to check and,
if necessary, modify the initial $\Delta t$.
Subsequently, the routine \api{Driver/Driver_computeDt} (usually invoked from \api{Driver/Driver_evolveFlash} {\em at the end}
of an iteration of the main evolution loop) is used to recompute $\Delta t$
for the next evolution step.

The implementation of \api{Driver/Driver_computeDt} can lower or increase the
time step. It takes various runtime parameters into consideration,
including
\rpi{Driver/dtmin},
\rpi{Driver/dtmax}, and
\rpi{Driver/tstep_change_factor}.
For the most part, however, \api{Driver/Driver_computeDt} calls on
\code{UNIT\_computeDt} routines of various code units to query those
units for their time step requirements, and usually chooses the smallest
time step that is acceptable to all units queried.

The code units that participate in this negotiation return a $\Delta t$,
and usually some additional information about which location in the
simulaton domain caused the reequired step to be as low as returned.
A unit's time step requirement can depend on the current state of the
solution as well as on further runtime parameters.
For example, the \code{dt} returned by \api{physics/Hydro/Hydro_computeDt}
depends on the state of density, pressure, and velocities (and possibly
additional variables) in the cells of the domain, as well as on
the runtime parameter \rpi{Hydro/cfl}.


\subsubsection{Generic time step limiting for positive definiteness}
\label{Sec:dr_posdef}
A generic method for limiting time steps, based on a requirement that certain (user-specified)
variables should never become negative, has been added in \flashx.
To understand the basic idea, think of each variable that this limiter is applied
to as a density of some quantity $q$. (Variables of \code{PER_MASS} type are actually converted to
their \code{PER_VOLUME} counterparts in the implementation of the method.)

The algorithm is based on examining the distribution of $q$ in neighboring cells,
and making for each cell a near-worst-case estimate of the rate of depletion $\dot q$ 
based on projected fluxes of $q$ out of the cell over its faces.
This can be applied to any variable, it is not required that the variable
represents any quantity that is actually advected as described.
See \tblref{Tab:dr_posdef parameters} for how to use.
This feature may be particularly useful when applied to \code{"eion"} in
multidimensional 3T simulations in order to avoid some kinds of ``negative ion temperature''
failures, as an alternative to lowering the \unit{Hydro} runtime parameter \rpi{Hydro/cfl}.


\begin{table}
\caption{\label{Tab:dr_posdef parameters}Runtime parameters for positive definiteness.}
\begin{center}
\begin{tabular}{lllp{3in}}
Parameter & Type  & Default & Description\\
\hline
%%\endfirsthead
%%\caption{\code{Driver} Unit parameters.} \\
%%Parameter & Type  & Default & Description \\
%%\hline
%%\endhead
\rpi{Driver/dr_usePosdefComputeDt} & boolean & .false. & Set to .true.\ to turn 
         positive-definite time step limiter on.\\ \\
\rpi{Driver/dr_numPosdefVars} & integer & 4  & Number of variables for which positive definiteness
        should be enforced\\ \\
\code{dr\_posdefVar\_N} & string & \code{"none"} & Name of Nth variable for positive-definite dt limiter\\ \\
\code{dr\_posdefDtFactor} & real & 1.0 & Scaling factor for dt limit from positive-definite time step limiter.
 Similar to CFL factor. If set to -1, use \rpi{Hydro/CFL} factor from \unit{Hydro}.\\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{comment}
\tblref{Tab:driver parameters}.

\begin{center}
\begin{longtable}{lllp{3in}}
\caption{\label{Tab:driver parameters}\unit{Driver} Unit parameters.}\\
Parameter & Type  & Default & Description\\
\hline
\endfirsthead
\caption{\code{driver} Unit parameters (continued).} \\
Parameter & Type  & Default & Description \\
\hline
\endhead
\code{nend} & integer & 100  & Maximum number of
        timesteps to take before
        halting the simulation\\ \\
\code{restart} & boolean & .false. & Set to .true. to restart
         the simulation from a
         checkpoint file\\ \\
\code{run\_number} & string & ``'' & Identification number for run\\ \\
\code{run\_comment}& string & ``'' & Identifying comment for run\\ \\
\code{log\_file}   & string & ``flash.log'' & Name of log file\\ \\
\code{tinitial} & real & 0. & Initial simulation time\\
\\
\code{tmax} & real  & 1.  & Maximum simulation time
                           to advance before halting the simulation\\
\\
\code{zinitial} & real & -1. & Initial simulation redshift (ignored if $< 0$;
                              used to set \code{tinitial} if $> 0$)\\
\\
\code{zfinal} & real & -2. & Final simulation redshift (ignored if $< 0$)\\
\\
\code{dtini} & real  & $10^{-10}$ & Initial timestep\\  \\
\code{dtmin} & real  & $10^{-10}$ & Minimum timestep\\ \\
\code{dtmax} & real  & $10^5$     & Maximum timestep\\ \\
\code{small} & real  & $10^{-10}$ & Generic small cutoff value
        for dimensionless positive definite
        quantities\\ \\
\code{smlrho} & real  & $10^{-10}$ & Cutoff value for density\\ \\
\code{smallp/e/t/u/x}
        & real  & $10^{-10}$ & Cutoff values for pressure, energy,
        temperature, velocity, and
        advected abundances\\ \\
\code{x/y/zmin} & real  & 0  & Minimum $x$, $y$, and $z$
        coordinates for grid\\ \\
\code{x/y/zmax} & real  & 1  & Maximum $x$, $y$, and $z$
        coordinates for grid\\ \\
\code{geometry}& string & \code{"cartesian"}  & Grid geometry ---
       valid values are \code{"cartesian"}, \code{"cylindrical"}, and \code{"spherical"} \\ \\
\code{iProcs} & integer & 0  & If set to 1, use gravity\\ \\
\code{iburn} & integer & 0  & If set to 1, use nuclear burning\\ \\
\code{iheat} & integer & 0  & If set to 1, use heating processes\\ \\
\code{icool} & integer & 0  & If set to 1, use cooling processes\\ \\
\code{wall\_clock\_time\_limit} & real & 604800  & Maximum simulation time in
 seconds \\ \\
\code{print\_tstep\_loc} & boolean & .false.  & when .true., it
 prints the $x$,$y$, and $z$ coordinates of the cell that is determining the
 timestep (for all limiters).  \\

\hline
\end{longtable}
\end{center}
\end{comment}

\begin{comment}
The alternative ``delta formulation'' drivers
(\code{driver/time\_dep/delta\_form}) modify a set of variables
containing the {\it change} in the solution during the timestep. The
change is only applied to the solution variables after all operators
have been invoked. This technique permits more general time
integration methods, such as Runge-Kutta methods, to be employed,
and it provides a more flexible method for composing operators.
However, only a few physics Units can make use of it as yet. More
details on the delta formulation drivers appear in \secref{Sec:New
driver Units}.
\end{comment}



\begin{comment}
\section{Delta-formulation and Strang-state driver Units}
\label{Sec:New driver Units}
These driver Units implement different explicit time
advancement algorithms. This usage is slightly different
than that of the default driver Unit, which does not directly implement a time
advancement algorithm; the default driver and hydro Units each implement parts
of the Strang splitting time advancement. In this section are listed the time
advancement tasks common to all of the alternative drivers. In the following
subsections, the details of each time advancement method will be described.

The three driver Units written in the delta formulation are
\code{euler1}, \texttt{rk3}, and \code{strang\_delta}. They make
appropriate calls to the physics Units and update the solution by
calling functions provided by the formulation Unit. The
\code{strang\_state} driver is written in the state-vector
formulation; it also calls the physics Units, but does not update
the solution. To use these Units, first choose the driver by
including {\it one} of the following lines in the \code{Units}
file:
\begin{quote} \texttt{
INCLUDE driver/time\_dep/delta\_form/euler1}\\ \texttt{INCLUDE
driver/time\_dep/delta\_form/rk3}\\ \texttt{INCLUDE
driver/time\_dep/delta\_form/strang\_delta}\\ \texttt{INCLUDE
driver/time\_dep/delta\_form/strang\_state} \end{quote} The
directory names for some of the alternative Units are misleading.
All the alternative time advancements are in a directory named
\code{delta\_form}, regardless of their formulation.

The time advancement Unit determines which formulation Unit
should be used; two instantiations are possible.
For \code{euler1}, \code{rk3}, or \code{strang\_delta}, specify
\begin{quote}
\code{INCLUDE formulation/state\_form/delta\_form}
\end{quote}
but for \code{strang\_state}
specify
\begin{quote}
\code{INCLUDE formulation/state\_form}
\end{quote}
The services provided by the formulation Unit for the delta
formulation are a superset of those provided for the state-vector
formulation, which explains the directory structure used.
For both instantiations, the formulation Unit contains
(i)~subroutines for updating the conserved and auxiliary
variables locally (on a block or on a face of a block)
given a local operator $L_{physics}(U)$ and (ii)~a parameter which declares
which formulation is being used.
For the delta formulation, the Unit also (iii)~declares
the global $\Delta U$ array and contains subroutines for accessing it
and (iv)~provides a subroutine to update the variables globally.

For delta formulation time advancements, the delta formulation driver Units
use the formulation Unit to hold and access the global $\Delta U$ array
and to update the solution.
In the state-vector formulation, the formulation Unit
is not directly used by the driver; instead, the physics Units call
the update subroutines that the formulation Unit provides.


The alternative driver Units discretize the left-hand side of
\begin{equation}
{\partial V \over {\partial t}} = (\hbox{spatial difference terms})
+ (\hbox{source terms})~. \label{Eqn:diffeq}
\end{equation}
The time advancement algorithm is contained in a subroutine
named \code{evolve}.
Each call to \code{evolve} advances the
solution through one timestep for the \code{euler1} and \code{rk3} Units
and through two timesteps for the \code{strang\_state} and \code{strange\_delta}
Units.
Each time advancement algorithm begins with a vector of primary variables $V^n$
at time $t^n$ and an associated set of auxiliary variables $W^n$.
Primarily through calls to other physics Units, \code{evolve}
applies a set of operations to
the variables to produce an updated vector $V^{n+1}$ at
time $t^{n+1} = t^n + \Delta t$.
Depending on the formulation, the time advancement may or may not
update the auxiliary variables -- in the state-vector formulation,
the other physics Units update them.


The distinction between $V$ and $W$ is that time-dependent differential
equations are solved to determine the primary variables.
The auxiliary variables are obtained from the primary variables
through algebraic relations.
Often the primary variables are the conserved variables $U$, and in
the rest of this section, $U$ will replace $V$.
However, the time advancement algorithms implemented do not require
this correspondence.

The time advancement algorithms are written generally, in that each differential
equation is treated in the same way. The distinction between the equations (for
example, between the $x$-momentum equation and the total energy equation) is
expressed in the other physics Units.
The time advancement algorithm does not need to know the identity
of the variables on which it operates, except possibly
to update the auxiliary variables from the primary variables,
but this update is handled by a call to a subroutine provided by
the formulation Unit.

\subsection{The \unit{euler1} Unit}

The \code{euler1} Unit implements the first-order, Euler explicit scheme
\begin{equation}
U^{n+1} = U^n + \Delta t L(U^n)~, \label{Eqn:euler_explicit}
\end{equation}
where $L(U)$ represents all of the physics Units.
The Euler explicit method is implemented in the delta formulation.
No runtime parameters are defined for this Unit.

At the beginning of a timestep, $\Delta U$ is set to zero. Each of
the physics Units is called with $U^n$ as the initial state and
adds its contribution to $\Delta U$. After all the physics Units
have been called, the global $\Delta U$ array holds $L(U^n)$.
\eqref{Eqn:euler_explicit} yields $U^{n+1}$. Finally, the auxiliary
variables are updated from the conserved variables with a call to
the global update subroutine provided by the formulation Unit.

Note that because all the physics Units start with the same initial state,
the order in which the physics Units are called does not
affect the results (except possibly through floating point
roundoff differences when contributing to $\Delta U$).

The set of steps, consisting of calls to physics Units, updating the
conserved variables, and updating the auxiliary variables, is
often called a {\it stage}.
The majority of the computational cost of a stage is in the
calls to the other physics Units; this component corresponds to
a ``function evaluation'' for ordinary differential equation solvers.
In the Euler explicit algorithm, there is one stage per timestep.

\subsection{The \unit{rk3} Unit}

Runge-Kutta schemes are a class of ordinary differential
equation solvers which are appreciated for their higher order of accuracy,
ease of implementation, and relatively low storage requirements.
There are many third-order Runge-Kutta methods; all require at least
three stages.
Most require at least three storage locations per primary variable, but the
one implemented in the delta formulation in Flash-X, derived by
Williamson (J. Comp. Phys. 35:48, 1980), requires only two.

The two storage registers will be referred to as $U$ and $\Delta U$.
The global solution vector $U$ holds $U^n$ at the beginning
of the timestep and then intermediate solutions $U^{(\cdot)}$ at the
end of each stage.
The manipulation of the global $\Delta U$ array is more complicated.
$\Delta U$ accumulates contributions from the
physics Units during a stage, but it also holds results from
previous stages; it is important to distinguish between the
results of the physics Units $L(U)$ and the quantity held in the
global $\Delta U$ array.
First the algorithm will be shown; then the usage of the
global $\Delta U$ array will be discussed.
For the equation
\begin{equation}
{\partial U \over {\partial t}} = L(U)~,
\end{equation}
Williamson's algorithm is, starting with $U^n$,
\begin{eqnarray}
U^{(1)}           &=& U^n     + {1  \over {3}} \Delta t
  \Big[L(U^n) \Big]
 \label{Eqn:rk3_s1} \\
U^{(2)}           &=& U^{(1)} + {15 \over {16}} \Delta t
  \Big[L(U^{(1)}) - {5 \over {9}} L(U^n) \Big]
 \label{Eqn:rk3_s2} \\
U^{n+1} = U^{(3)} &=& U^{(2)} + {8  \over {15}} \Delta t
  \Big[L(U^{(2)}) - {153 \over {128}} \Big( L(U^{(1)}) - {5 \over {9}} L(U^n)
  \Big) \Big]~.
 \label{Eqn:rk3_s3}
\end{eqnarray}
$U^{(m)}$ is the result of the $m$-th stage, and the auxiliary variables are
updated each time a new $U^{(m)}$ is computed.

The algorithm is implemented using the following steps to attain the
low storage. At the beginning of the timestep, $\Delta U$ is set to
zero. During the first stage each physics Unit contributes to
$\Delta U$, so after all have contributed, $\Delta U$ holds the
bracketed term in \eqref{Eqn:rk3_s1}, $L(U^n)$. $U^{(1)}$ is then
computed using \eqref{Eqn:rk3_s1}. Stage~1 is completed by
multiplying $\Delta U$ by $-5/9$, which is required for the
following stages. The process is repeated for stage~2: after the
physics Units have contributed, $\Delta U$ holds the bracketed
term in \eqref{Eqn:rk3_s2}; $U^{(2)}$ is computed by
\eqref{Eqn:rk3_s2} and stored in $U$; then $\Delta U$ is multiplied
by $-153/128$. Stage~3 is similar, but ends after $U^{(3)} =
U^{n+1}$ is computed and stored. It is critical that the only
changes made to the $\Delta U$ array are those just listed; no
physics Unit should change the value of $\Delta U$, except to add
its contribution, and since $\Delta U$ holds information from
previous stages, it should not be reset to zero except at the
beginning of the timestep. No runtime parameters are defined for
this Unit.

\subsection{\unit{strang\_state} and \unit{strang\_delta} Units}

The second-order accurate splitting method (Strang 1968) is
attractive because of its low memory requirements.
The algorithm is based on the operator splitting approach,
in which a set of simple subproblems is solved rather than a single,
complicated problem.
Each subproblem typically accounts for one term in a system
of partial differential equations, representing a particular
type of physics for which an appropriate (specialized)
numerical method is available.
If all the subproblems are computed to at least second-order accuracy, the basic operator splitting method is first-order accurate; however,
the Strang splitting scheme recovers second-order accuracy over two timesteps.
In the first timestep, the subproblems are solved in a given sequence.
Second-order accuracy is obtained by reversing the sequence in the
second timestep.

A key feature of the operator splitting approach is that the output of
one subproblem is the input to the next subproblem. This allows for an
implementation that globally, stores only the current solution, but it
can also cause problems including accuracy losses due to decoupling
various physical effects (splitting errors) and difficulties
implementing boundary conditions.  In practice it has been found that
splitting errors are reduced when the subproblems are ordered in
increasing stiffness, \ie, the stiffest subproblem is solved
last in the sequence; this has recently been supported by numerical
analysis (Sportisse 2000).

Two driver Units implement an algorithm similar to the Strang
splitting time advancement.
Since the sequence is not exactly reversed in the second step
compared to the first, the algorithm is not the true Strang splitting.
However, the nuclear burning source terms
are very stiff, and there are sound arguments for computing them
last.
The \unit{strang\_state} Unit implements the algorithm in the
state-vector formulation and is recommended for ``production''
runs due to its low memory requirements.
The \unit{strang\_delta} driver is implemented in the delta
formulation and is provided for testing and comparison.
For both versions, one call to \fcn{evolve} (which implements
the time advancement algorithm) advances the solution
from $t^n$ to $t^{n+2}$, \ie, over \emph{two} timesteps.

In the \unit{strang\_state} driver, the sequence of calls to
physics Units in the first timestep is
\begin{quote}
hydro($x$-sweep) \\
hydro($y$-sweep) \\
hydro($z$-sweep) \\
gravity        \\
source terms
\end{quote}
In the second timestep, only the order of the hydro calls is reversed
\begin{quote}
hydro($z$-sweep) \\
hydro($y$-sweep) \\
hydro($x$-sweep) \\
gravity        \\
source terms
\end{quote}
Mesh refinement and derefinement are executed only after the second step,
not between the two steps;
also the timestep is held constant for the two steps.
The $y$- and $z$-sweeps of hydro are not called unless that dimension is
included in the simulation.
The same algorithm is used in the \unit{strang\_delta} Unit,
but after each call to a physics Unit, a call to a subroutine
is necessary to update the solution. When the \unit{strang\_state} driver
is used, these calls are made by each physics Unit.
No runtime parameters are defined for either Unit.

\subsection{The \code{formulation} Units}
\label{Sec:new formulation Units}

The purposes of this Unit class are
\begin{enumerate}
\item To provide functions usable by physics Units and driver Units
to update the solution locally (on a block or on a face of a block) or
globally (on all blocks).
\item If needed by the time advancement (driver) Unit, to provide
storage space for the global $\Delta U$ array and functions to access
it.
\end{enumerate}
The alternative time advancement methods (drivers) are implemented in
either the state-vector or delta formulations.  There are two
corresponding instantiations of the formulation Unit.  In the
state-vector instantiation, only the local update functions in
item~(1) are provided; drivers in the state-vector formulation do not
require any other services.  The delta instantiation provides both
local and global update functions and global $\Delta U$ array storage
as required by drivers in the delta formulation.

The services provided to delta formulation drivers are a superset of
those provided to drivers in the state-vector formulation, and the
directory structure is used to express that.  The
\code{/formulation/state\_form} directory contains the local update
subroutines and a version of \texttt{formulation\_Unit} suitable
for the state-vector instantiation. \texttt{formulation\_Unit}
defines a Unit in the \Fortran90 sense as opposed to the Flash-X
hierarchy sense.  The \texttt{formulation/state\_form/delta\_form}
directory contains the global update subroutine and the version of
\code{formulation\_Unit} required for the delta instantiation.

When \code{/formulation/state\_form} is specified in the
\code{Units} file, the local update functions and the first
\code{formulation\_Unit} are built into the executable, as
appropriate for drivers in the state-vector formulation; when
\code{/formulation/state\_form/delta\_form} is specified in the
\code{Units} file, the local update functions, the global update
function, and the second version of \texttt{formulation\_Unit} are
used in the executable, as required by drivers in the delta
formulation.  This use of the Flash-X code framework and directory
hierarchy allows static allocation of the global $\Delta U$ array
when needed but saves that memory when not.  At the same time, it
allows local update functions to be used by both state-vector and
delta formulations without duplicating code.

Currently the update functions apply only to the particular variable
sets described. The local update functions must be given the (old)
conserved variables in the order $\rho_1, \cdots,
\rho_{ionmax}$, $\rho u$, $\rho v$, $\rho w$, $\rho E$, and they
store in the database $X_1, \cdots, X_{ionmax}$, $\rho$, $P$, $T$, $\gamma$,
$u$, $v$, $w$, and $E$.  The mapping from the conserved variables to the
database variables is not general; it is specific to the variables
just listed.  Variables other than those specifically listed will not
be updated, and their influence on the variables just listed will be
ignored.  Development of more flexible update routines is underway.
However, changes will most likely be internal to the local and global
update functions, and the organization of these Units is not
expected to change.

\subsubsection{State-Vector Instantiation}

In this subsection the local update functions, named \texttt{
du\_update\_block}, \code{du\_update\_xface},
\code{du\_update\_yface}, and \code{du\_update\_zface}, are
described. These subroutines accept local arrays of conserved
variables and their changes as inputs, compute updated conserved
variables, compute auxiliary variables from algebraic relations
(with the aid of appropriate equation of state calls), and store the
updated variables in the database.

These subroutines accept the block number, a local $\Delta U$, local
conserved variables $U$, the timestep $\Delta t$, and a scalar factor
$c$, all as passed arguments.  The face update routines also accept an
index specifying which grid plane to update.
The conserved variables are updated by
\begin{equation}
U^\textnormal{new} = U^\textnormal{old} + c \Delta t \Delta U~.
\label{Eqn: Conserved variables and fluxes}
\end{equation}
The factor $c$ allows an update to an intermediate time between $t^n$
and $t^{n+1}$, often required by Runge-Kutta time advancement methods;
it is intended for use by drivers in the delta formulation through the
global update subroutine.

From the updated conserved variables, all variables stored in the
database are computed.  The density $\rho$ and species mass
fractions $X_s$ are obtained from the species
densities $\rho_s$. The velocity components~$u$, $v$, and $w$ and the
total energy per unit mass~$E$ are computed from the momenta and
total energy per unit volume, respectively, by dividing by $\rho$. The
internal energy $\epsilon$ is calculated by subtracting the kinetic
energy per unit mass $(u^2 + v^2 +w^2)/2$ from~$E$.  The temperature $T$,
pressure $P$, and ratio of specific heats $\gamma$ are obtained
through a call to the equation of state, for which $\rho$, $X_s$, and
$\epsilon$ are inputs.

Finally, the updated variables are stored in the variable database.
The variables stored are $X_s$, $\rho$, $P$, $T$, $\gamma$, $u$, $v$,
$w$, and $E$.  Only the interior cells of a block or face are updated;
for all guard cells, zeros are stored for all updated variables. None
of the calculations described above are executed for the guard cells.

For the state-vector formulation, there are only a few tasks for the
\Fortran90 Unit \code{formulation\_Unit}. First, it defines a
\FORTRAN\ logical parameter \code{delta\_formulation} to be
\code{.false.}. This parameter is designed to be accessed by physics
Units. When \code{.false.}, it indicates that each physics Unit
should update the solution; while the local update routines just
described are recommended for this purpose, there is no requirement
that they be used.  Second, \texttt{formulation\_Unit} defines
several parameters for sizing arrays and a set of integers (indices)
used to access the variable database; these are used by the local
update subroutines.

In the state-vector instantiation, \code{formulation\_Unit} does
not declare the global $\Delta U$ array. It does define some
functions which are used to access that array, but in this
instantiation they do not perform any operations -- they are `stub'
functions. The reason for defining them is as follows. If a physics
Unit is written so that either the state-vector or delta
formulation can be used, it must include calls to functions which
give access to the global $\Delta U$ array. When the state-vector
formulation is used these calls are not made, but many compilers
raise errors when these functions are not defined. By defining them
in this instantiation of \texttt{formulation\_Unit}, such errors
are avoided.  The stub functions are {\it contained}, in the
\Fortran90 sense, in \code{formulation\_Unit}. The local update functions
are not contained in \texttt{formulation\_Unit}, although they
directly access the array sizing parameters and database indices
therein.

\subsubsection{Delta Instantiation}

In this section the global update subroutine \code{du\_update} and the
version of \code{formulation\_Unit} used for the delta formulation are
described. The global update routine is a wrapper to the local update
subroutine \code{du\_update\_block}.  Two arguments, $c$ and $\Delta
t$, are passed into \code{du\_update}. For each block, it gets $\rho$,
$X_s$, $u$, $v$, $w$, and $E$ from the database; computes the (old)
conserved variables from these; gets the $\Delta U$ for the block from
the global $\Delta U$ array; and calls \code{du\_update\_block}.
Recall that \code{du\_update\_block} computes the updated variables and
stores them in the database.

For the delta instantiation, \code{formulation\_Unit} defines the
same array-sizing parameters and database indices as in the
state-vector instantiation. However, it defines the parameter
\texttt{delta\_formulation} to be \code{.true.}, indicating to the
physics Units that their contributions should be added to the
global $\Delta U$ array. The delta instantiation of
\code{formulation\_Unit} statically allocates the global $\Delta
U$ array and defines several functions to manipulate it. Each
element of the global $\Delta U$ array is set to zero by
\code{du\_zero}. A physics Unit can add its local $\Delta U$ for a
block to the global array by calling \texttt{
du\_block\_to\_global}; the subroutines
\code{du\_xface\_to\_global}, \code{du\_yface\_to\_global} and
\code{du\_zface\_to\_global} do the same for faces (slices) of a
block.  These subroutines are contained in
\code{formulation\_Unit} and are the actual, working versions of
the stub functions defined in the state-vector instantiation.

The global $\Delta U$ array is a public, Unit-scope variable in the
\Fortran90 sense. The \code{du\_update} subroutine is not contained in
\code{formulation\_Unit} but can access the array-sizing parameters
and database indices in the Unit. It can also access the global
$\Delta U$ array directly and is the only subroutine not contained in
\code{formulation\_Unit} allowed to do so.
\end{comment}


